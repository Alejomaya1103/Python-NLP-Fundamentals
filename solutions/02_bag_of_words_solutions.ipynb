{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c37c33-fd3c-4b3a-bbf5-661d29798421",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Part 2 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f340930-458d-4c63-939f-91ef765b8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b47991-a10b-4f6d-b4e9-188a4644d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to import tweets\n",
    "tweets_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984af902-74f5-4244-a511-08d33bdf6776",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 1: Apply a Text Cleaning Pipeline\n",
    "\n",
    "Write a function called `preprocess()` that performs the following steps on a text input:\n",
    "* Step 1: Lowercase text.\n",
    "* Step 2: Replace the following patterns with placeholders:\n",
    "    * URLs &rarr; ` URL `\n",
    "    * Digits &rarr; ` DIGIT `\n",
    "    * Hashtags &rarr; ` HASHTAG `\n",
    "    * Tweet handles &rarr; ` USER `\n",
    "* Step 3: Remove extra blankspaces.\n",
    "\n",
    "Here are some hints to guide you through this challenge! \n",
    "\n",
    "* For Step 1, recall from Part 1 that a string method called [`.lower()`](https://docs.python.org/3.11/library/stdtypes.html#str.lower) will do the job of lowercasing text input.\n",
    "* We have integrated Step 2 in a function called `placeholder`. Run the next cell to import it into your notebook, and you can use it just like any other functions.\n",
    "* For Step 3, we have provided the regex pattern for identifying whitespace characters as well as the correct replacement for extraneous whitespaces. \n",
    "\n",
    "Run your `preprocess()` function on `example_tweet` (three cells below), and when you think you have it working, apply it to the entire `text` column in the tweets DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0a69c0-d541-4776-a4b7-9b5250a11db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m placeholder\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c3fe7-c67d-4636-96e2-00c59d2e9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = ' '\n",
    "\n",
    "def preprocess(text):\n",
    "    '''Create a preprocess pipeline that cleans the tweet data.'''\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace patterns with placeholders\n",
    "    text = placeholder(text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea8f47-58f0-4547-a2f0-dddb62940e15",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 2: Lemmatize the Text Input\n",
    "\n",
    "Recall from Part 1 that we introduced using `spaCy` to perform lemmatization, i.e., removing morphological affixes on words. With lemmatization, we keep only word stems in texts, which presumbaly should capture the core meaning of the text. \n",
    "\n",
    "Now let's implement lemmatization on our tweet data, and pass the lemmatized text to create a third DTM. \n",
    "\n",
    "Complete the function `lemmatize_text`. It requires a text input, and the returned output is the same text except this time lemmas of all tokens. There are several steps we need to consider to complete this function:\n",
    "- Initialize a list to hold lemmas\n",
    "- Apply the `nlp()` pipeline to input text\n",
    "- Iterate over tokens in the processed text, and retrieve lemma of the token\n",
    "    - HINT: lemma is one of the linguistic annotations that the `nlp` pipeline returns. We can use `token.lemma_` to access the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88bc0e-2666-44c0-b75f-611cb6457602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bd8e1-a461-4f25-84aa-de95eb5d1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    '''Lemmatize the text input with spaCy annotations.'''\n",
    "    \n",
    "    # Apply the nlp pipeline to input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Append the token lemma to list and join them into a single string\n",
    "    text_lemma = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbabf36-07ff-4ad0-bf8f-bc94cf32da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to an example tweet\n",
    "print(tweets.iloc[101][\"text_processed\"])\n",
    "print(f\"{'='*50}\")\n",
    "print(lemmatize_text(tweets.iloc[101]['text_processed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eea277-8f00-42c9-9a2e-5a6d802378a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while!\n",
    "tweets['text_lemmatized'] = tweets['text_processed'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0592b67-64a8-4b3b-b22e-4bd6731a3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the preprocessed tweet\n",
    "print(tweets['text_processed'].iloc[101])\n",
    "print(f\"{'='*50}\")\n",
    "# Print the lemmatized tweet\n",
    "print(tweets['text_lemmatized'].iloc[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfec9d6-3992-4737-ab8c-a086dd96bcdc",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 3: Words with Highest Mean TF-IDF scores\n",
    "\n",
    "So we have got tf-idf values for each term in each document, does that inform us anything about our data? Instead of focusing on tf-idf value of any particular word, let's take a step back. Is there any word to be particularly informative for positive/negative tweets? Let's gather the indices to all positive/negative tweets, and calculate the mean tf-idf scores of words appear in positive/negative tweets. \n",
    "\n",
    "We've provided the following starter codes to scaffold:\n",
    "- Use boolean masks to select tweets that have positive/negative sentiments, retrieve the indices, and assign them to `positive_index`/`negative_index`\n",
    "- Select positive/negative tweets in the tfidf dataframe, and take the mean tf-idf values across the documents, sort the mean values in descedning order, and get the top 10 terms. \n",
    "\n",
    "After you've completed the following two cells, plot the words having the highest mean tf-idf scores for each subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5c819-e002-415c-a8fb-c520e1a37e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             min_df=2,\n",
    "                             max_df=0.95,\n",
    "                             max_features=None)\n",
    "\n",
    "# Fit and transform \n",
    "tf_dtm = vectorizer.fit_transform(tweets['text_lemmatized'])\n",
    "\n",
    "# Create a tf-idf dataframe\n",
    "tfidf = pd.DataFrame(tf_dtm.todense(),\n",
    "                     columns=vectorizer.get_feature_names_out(),\n",
    "                     index=tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c4e33-82c9-437c-bffc-3b0921a7d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the boolean masks \n",
    "positive_index = tweets[tweets['airline_sentiment'] == 'positive'].index\n",
    "negative_index = tweets[tweets['airline_sentiment'] == 'negative'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09bed5-ce7f-41e3-a56c-348d3b0770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following two lines\n",
    "pos = tfidf.loc[positive_index].mean().sort_values(ascending=False).head(10)\n",
    "neg = tfidf.loc[negative_index].mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487448f8-1fd5-4909-a41e-977f23cec4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.plot(kind='barh', \n",
    "         xlim=(0, 0.18),\n",
    "         color='cornflowerblue',\n",
    "         title='Top 10 terms with the highest mean tf-idf values for positive tweets');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b5f35-4965-47d0-8340-b932ee452df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.plot(kind='barh', \n",
    "         xlim=(0, 0.18),\n",
    "         color='darksalmon',\n",
    "         title='Top 10 terms with the highest mean tf-idf values for negative tweets');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
