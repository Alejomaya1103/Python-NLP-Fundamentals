{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e7ea21-6437-48e8-a9e4-3bdc05f709c9",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Preprocessing\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Apply string manipulations to text data. \n",
    "* Use both default python packages and NLP-specific packages to do text cleaning. \n",
    "* Learn to build a pipeline of preprocessing and tokenization. \n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "ðŸ”” **Question**: A quick question to help you understand what's going on.<br>\n",
    "ðŸ¥Š **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "ðŸ’¡ **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "ðŸŽ¬ **Demo**: Showing off something more advanced â€“ so you know what Python can be used for!<br> \n",
    "\n",
    "### Sections\n",
    "1. [Importing Text Data](#section1)\n",
    "2. [Text Cleaning](#section2)\n",
    "3. [Tokenization](#section3)\n",
    "4. [Demo: Powerful Features of `spaCy`](#demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8914f4-9783-4661-9cc9-32daca53e1fd",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# Importing Text Data \n",
    "\n",
    "Text data we want to analyze will be stored in external files that need to be imported. These files will generally be text files (`.txt`) or comma separated value files (`.csv`).\n",
    "\n",
    "All the data used in this notebook are stored in a `data` folder that we need to access. \n",
    "\n",
    "To access a specific text file, we need to specify its file path. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6911f44-3181-4d51-b996-407b01b9f98a",
   "metadata": {},
   "source": [
    "## Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "366544be-4b56-4ed2-ba8e-aa4e41d5f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../data/sowing_and_reaping.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462052f-766c-434c-a262-efa7326d61c0",
   "metadata": {},
   "source": [
    "We'll first start by importing \"Sowing and Reaping\" by Frances Harper, which is stored in a text file. \n",
    "\n",
    "Python has built-in functionality for importing text files: `open()`, see documentation [here](https://docs.python.org/3/library/functions.html#open). \n",
    "\n",
    "The function typically takes in two arguments: the file path and the mode in which the file is opened. The default mode is `r`, which stands for \"reading in text data\".  \n",
    "\n",
    "Let's store the text file in an object called `raw_text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3dc4536-5e6c-4052-a44d-c36760714925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the text\n",
    "with open(text_path, 'r') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbba12-bf88-44e7-8857-41df7eed67e0",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Tip**: When using the `with` statement to open a file, it will automatically be closed once you're done with it. If you open a file using the `open()` function individually and assign it to a variable, remember to close the file explicitly using the `close()` function when you're finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8e3b1-f92e-4525-a5ba-f396fb61271a",
   "metadata": {},
   "source": [
    "Let's remove the front and end matter for better preprocessing later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ff4469c-6a66-44bb-bd82-f20455497141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the front and end matter\n",
    "sowing_and_reaping = raw_text[1114:684814]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ef3f4-1c0c-4c79-918f-72175a1292d3",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 1: String Manipulation\n",
    "\n",
    "* What type of object is `sowing_and_reaping`?\n",
    "* How many characters are in `sowing_and_reaping`?\n",
    "* How can we get the first 1000 characters of `sowing_and_reaping`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e693ebf0-6d66-4650-b825-e27f6860a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e1de5-7cc6-4861-9ca9-0b58996d25f2",
   "metadata": {},
   "source": [
    "## Comma Separated Value (CSV) Files\n",
    "\n",
    "Often, we may have data stored in \"dataframes\" or \"tables\", which consists of many records (rows), each containing several features (columns). Among the features is likely a text column which contains the text of interest. \n",
    "\n",
    "These dataframes are often found as Comma Separated Value (CSV) files (also as tab separated value (TSV) files). In either case, there is some \"delimiter\" (i.e., a comma or tab) which helps separate entries from each other.\n",
    "\n",
    "The `pandas` package is the best package for dealing with dataframes in Python, and this package comes with its own function for reading CSV files. \n",
    "\n",
    "For example, let's read in a csv file containing many Tweets about airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c75449af-a3ad-48df-a64f-1325d2ef5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Use pandas to import Tweets\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "\n",
    "# Specify the separator to be comma\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adf1de-5ed4-49f5-a581-70d38fb1a3d1",
   "metadata": {},
   "source": [
    "ðŸ”” **Question**: Let's take a look at the dataframe. Which column points to the tweet itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb6e3107-9f0a-414b-8267-1d5fbb09e246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201a3de-aaf6-4c21-97c8-5ac38517e0b5",
   "metadata": {},
   "source": [
    "Let's take a look at some of the Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c32d507-ff14-4a2b-b7ae-613f3f76c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n"
     ]
    }
   ],
   "source": [
    "# Use iloc to extract some tweets\n",
    "print(tweets['text'].iloc[0])\n",
    "print(tweets['text'].iloc[1])\n",
    "print(tweets['text'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972dfc7-ddb4-486e-ac48-d5a7593f7b91",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 2: Reading in Many Files\n",
    "\n",
    "The `data` folder contains another folder called `amazon`, which contains many `csv` files of Amazon reviews. \n",
    "\n",
    "Use a `for` loop to read in each dataframe. Do the following:\n",
    "\n",
    "* We've provided a path to the `amazon` folder, and a list of all the file names within the folder using the `os.listdir()` function.\n",
    "* Iterate over all these files, and import them using `pd.read_csv()`. You will need to use `os.path.join()` to create the correct path. Additionally, you need to provide `pandas` with the column names since they are not included in the reviews. We have create the `column_names` variable for you.\n",
    "* Extract the text column from each dataframe, and add then to the `reviews` list. \n",
    "* How many totals reviews do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d9fa08c-7e28-4c3f-ab0c-1d1ab5ad4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The os package has useful tools for file manipulation\n",
    "import os\n",
    "\n",
    "# Amazon review folder\n",
    "amazon_path = '../data/amazon'\n",
    "\n",
    "# List all the files in the amazon folder\n",
    "files = os.listdir(amazon_path)\n",
    "\n",
    "# Column names for each file\n",
    "column_names = ['id',\n",
    "                'product_id',\n",
    "                'user_id',\n",
    "                'profile_name',\n",
    "                'helpfulness_num',\n",
    "                'helpfulness_denom',\n",
    "                'score',\n",
    "                'time',\n",
    "                'summary',\n",
    "                'text']\n",
    "\n",
    "# Add each review text to this list\n",
    "reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb680f3e-b82a-411c-aff0-79d2a6dfbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # Check that the file is actually a CSV file\n",
    "    if os.path.splitext(file)[1] == '.csv':\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "        text = ''\n",
    "        reviews.extend(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce98479-c234-41bc-a2da-c90ab822b421",
   "metadata": {},
   "source": [
    "There are other file types which you may come across: `json`, `xml`, `html`, etc. There are packages you can use to import each other these. The main challenge, in most cases, is dealing with multiple files, and extracting the actual text you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e88f2b-bdaa-4c28-ad13-4e790ccb6827",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Text Cleaning\n",
    "\n",
    "Text cleaning is a catch-all term for the process of performing relatively simple tasks in order to normalize our code. Language can be messy, so there's a variety of different things depending on your use case.\n",
    "\n",
    "## Regular Expressions\n",
    "\n",
    "Before we dive into the specific text cleaning processes, let's briefly introduce regular expressions. We do this here since many text cleaning steps may require regular expressions, and many NLP libraries heavily use them under the hood.\n",
    "\n",
    "Regular expressions (regexes) are a powerful way of searching for specific string patterns in large corpora. They have an infamously steep learning curve, but are very efficient when you get a handle on them.\n",
    "\n",
    "Our goal in this workshop is not to provide a deep (or even shallow) dive into regexes; instead, we want to expose you to them so that you're better prepared to do deep dives in the future.\n",
    "\n",
    "Regex testers are a useful tool in both understanding and creating regex expression. An example is this [website](https://regex101.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "93887a67-b8a5-46cb-96ab-3b1b4fc5e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex\n",
    "import re\n",
    "\n",
    "# Create a text pattern\n",
    "pattern = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd9690d3-c12f-4a2c-bcc7-cfa3f3523949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n",
      "This is a not a test.\n"
     ]
    }
   ],
   "source": [
    "# Create a text string\n",
    "test_string = 'This is a test.'\n",
    "\n",
    "# Find tokens\n",
    "tokens = re.findall(pattern=pattern, string=test_string)\n",
    "print(tokens)\n",
    "\n",
    "# Replace tokens\n",
    "replaced = re.sub(pattern=pattern, repl='not a test', string=test_string) \n",
    "print(replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336ea3a-dd51-48a1-bf58-095298afdeb4",
   "metadata": {},
   "source": [
    "This is nice, but we could have done this somewhat easily with basic Python `string` functions. Let's try something more interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "244236dc-60b9-490c-98b7-33a6af5cb2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word pattern matcher\n",
    "pattern = r'\\w+'\n",
    "re.findall(pattern, test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60923f-b981-4ad9-8777-0951105736d8",
   "metadata": {},
   "source": [
    "ðŸ”” **Question**: What did this do? Use the regex website to confirm your guess!\n",
    "\n",
    "For now, we won't go much further than this, but there are many resources online to continue learning about regexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebae34-7b23-49a9-8aad-8f49cb55f320",
   "metadata": {},
   "source": [
    "## Lowercasing\n",
    "\n",
    "While there is often information in the \"casing\" of words (e.g., whether text is lowercase or uppercase), we often don't work in a regime where we're able to properly leverage this information. So, a common text cleaning step is to lowercase all text, in order to simplify our analysis.\n",
    "\n",
    "We can easily do this with the built-in string function `lower()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a07353a-0dfc-4438-ade1-d2fee7a3305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all text in sowing and reading\n",
    "sowing_and_reaping_lower = sowing_and_reaping.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e2cab72-6002-4861-af38-d0b67296eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hear that John Andrews has given up his saloon; and a foolish thing\n",
      "it was. He was doing a splendid business. What could have induced him?\"\n",
      "\n",
      "\"They say that his wife was bitterly opposed to the busin\n",
      "\n",
      "\n",
      "i hear that john andrews has given up his saloon; and a foolish thing\n",
      "it was. he was doing a splendid business. what could have induced him?\"\n",
      "\n",
      "\"they say that his wife was bitterly opposed to the busin\n"
     ]
    }
   ],
   "source": [
    "print(sowing_and_reaping[:200])\n",
    "print('\\n')\n",
    "print(sowing_and_reaping_lower[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39654022-c93b-4bf5-9a63-efaf2b8125c3",
   "metadata": {},
   "source": [
    "## Removing Punctuation\n",
    "\n",
    "Sometimes, you might want to keep only the alphanumeric characters (i.e. the letters and numbers) and ditch the punctuation. This becomes less common when we consider more advanced NLP algorithms. \n",
    "\n",
    "Note that in many cases, you may do this step **after** tokenization, which we will discuss in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9aee43b2-67c4-41b6-af4b-5a3128dab57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Import the default list of punctuation from string function\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976ad06-bd40-4732-80c7-1b530374dffe",
   "metadata": {},
   "source": [
    "We can write a list comprehension, iterating over characters in a string of text and identifying and removing all punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66e1da6f-0c1b-4588-a310-028c18fcad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weve got quite a bit of punctuation here dont we Python DLab\n"
     ]
    }
   ],
   "source": [
    "punctuation_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"\n",
    "no_punctuation = ''.join([char for char in punctuation_text if char not in punctuation])\n",
    "print(no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19c139-5f87-4ba1-b6ba-7c24b1dbc7e8",
   "metadata": {},
   "source": [
    "As you can see, removing punctuation before tokenization gives us weird expressions such as `weve`, which again illustrates the point of applying this step after tokenization, which will separate `we` and `'ve` out in the first place. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f9bba-e2a5-4839-b017-f979a09752a1",
   "metadata": {},
   "source": [
    "## Stripping Blank Spaces\n",
    "\n",
    "Removing blank space is a common step, as we might come across text with extraneous blank space. This is particularly common when text is imported from messy places, like webpages.\n",
    "\n",
    "Python has a built-in function `strip()` to deal with blank space on the **ends** of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71d42f85-71fd-4fb3-9c29-c24c41188fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = ' Hello! '\n",
    "string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3d2ef-3111-43f4-b77d-08d9363a0756",
   "metadata": {},
   "source": [
    "What about within text? We will need to use a regular expression for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cca33c78-f8bf-438f-9737-f0a89c30c470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in and print an example file\n",
    "example1_path = '../data/example1.txt'\n",
    "\n",
    "with open(example1_path, 'r') as file:\n",
    "    example1 = file.read()\n",
    "    \n",
    "print(example1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62654c4c-8820-49ce-b4a8-0c04ccb75d70",
   "metadata": {},
   "source": [
    "As you can see, `strip()` only removes the blanks at the ends. Note the blank lines at both ends are no longer present.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53e94cb2-49c1-4be2-8496-6dd124d90188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n"
     ]
    }
   ],
   "source": [
    "# Apply strip()\n",
    "print(example1.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4579c5-a607-4492-bcb8-dde8a1a76d3e",
   "metadata": {},
   "source": [
    "To remove blank spaces within the text, we can write a pattern `r'\\s+`, which recognizes one or more blank spaces, and use `re.sub()` to replace any within-text blank space(s) with one blank space. \n",
    "\n",
    "You'll notice that using this pattern, we keep the word boundaries as usual but only removing redundent blank spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "083b1553-dc87-4150-b857-76c2f0df8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines. The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won\\'t catch it in the middle, for example, in this sentence. Once again, regular expressions will help us with this.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pattern for blank spaces\n",
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = ' '\n",
    "\n",
    "# Use re.sub to replace blank spaces with empty\n",
    "clean_text = re.sub(blankspace_pattern, blankspace_repl, example1)\n",
    "clean_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a168e-e2e0-46c0-b794-fa0ecae400dc",
   "metadata": {},
   "source": [
    "## Removing URLs, Hashtags, and Numbers\n",
    "\n",
    "Text containing non-alphabetic symbols may have additional meaning beyond simply using punctuation or numbers. For example, text may contain URLs, hashtags, or numbers. Each of these are informative in their own right.\n",
    "\n",
    "However, we rarely care about the exact URL used in a tweet. Similarly, we might not care about specific hashtags, or the precise number used. While, we could remove them completely, it's often informative to know that there *exists* a URL, hashtag, or number.\n",
    "\n",
    "So, we replace individual URLs, hashtags, and numbers with a \"symbol\" that preserves the fact these structures exist in the text. It's standard to just use the strings \"URL\", \"HASHTAG\", and \"DIGIT\".\n",
    "\n",
    "Since these types of text often contain precise structure, they're an apt case for using regular expressions. Let's apply these patterns to the Tweets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8069337-4a39-4437-8710-f9b5c58335f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel http://t.co/ahlXHhKiyn\n"
     ]
    }
   ],
   "source": [
    "# Get a Tweet with a URL in it\n",
    "url_tweet = tweets['text'].iloc[13]\n",
    "print(url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc1fa10d-e546-4e61-89e5-adf53c7e6262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel  URL \""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL \n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "url_repl = ' URL '\n",
    "re.sub(url_pattern, url_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40416f9e-4052-425f-9a00-94a3b0a94e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your HASHTAG  HASHTAG  skies again! U take all the HASHTAG  away from travel http://t.co/ahlXHhKiyn\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hashtag\n",
    "hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "hashtag_repl = ' HASHTAG '\n",
    "re.sub(hashtag_pattern, hashtag_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fba96052-a24e-46ca-8d99-60e47b485c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica help, left expensive headphones on flight 89 IAD to LAX today. Seat 2A. No one answering L&amp;F number at LAX!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica help, left expensive headphones on flight  DIGIT  IAD to LAX today. Seat  DIGIT A. No one answering L&amp;F number at LAX!'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Digits\n",
    "digit_tweet = tweets['text'].iloc[32]\n",
    "print(digit_tweet)\n",
    "digit_pattern = '\\d+'\n",
    "digit_repl = ' DIGIT '\n",
    "re.sub(digit_pattern, digit_repl, digit_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a288b9a-a33a-48be-bac4-93ffd39318ec",
   "metadata": {},
   "source": [
    "What other kinds of text strings can you think of that we might want to replace?\n",
    "\n",
    "Natural language is complex, and so there may be use cases where we might need specialized packages for preprocessing or removing text. For example, the [`emoji` package](https://pypi.org/project/emoji/) may be useful for social media text. The [`textacy` package](https://textacy.readthedocs.io/en/latest/) also provides useful preprocessing tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5e64f-c6a0-447b-973e-a48e9eb80ade",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 3: Text Cleaning with Multiple Steps\n",
    "\n",
    "In Challenge 1, we imported many Amazon reviews, and stored them in a variable called `reviews`. Each element of the list is a string, representing the text of a single review. For each review:\n",
    "\n",
    "* Replace any URLs and digits.\n",
    "* Make all characters lower case.\n",
    "* Strip all blankspace.\n",
    "\n",
    "Keep in mind: the order in which you do these steps matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d599284-2392-488c-a4a8-dad5eeb50f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc7c15-1bc7-47cd-b668-ab57634c8b04",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# Tokenization\n",
    "\n",
    "One of the most important steps in text analysis is tokenization. This is the process of breaking down the text into \"tokens\", which are distinct chunks that we recognize as unique in whatever corpus we're working in.\n",
    "\n",
    "Let's start by importing an example file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cd9d972-ae66-4415-8ce6-359cb9db0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this little example, we're going to see some of the problems that regularly appear in tokenization. Tokenization may seem simple, but it's harder than it first appears. Why is it so hard? Punctuations, contractions (like don't, won't and would've) get in the way. What do you do when you have #hashtags, @TwitterHandles, or https://urls.com? Different packages will make different decisions on when to split text apart, and when not to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import an example file\n",
    "example2_path = '../data/example2.txt'\n",
    "\n",
    "with open(example2_path) as file:\n",
    "    example2 = file.read()\n",
    "    \n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b402ed3-de7f-4e16-ade6-444c9e74f8eb",
   "metadata": {},
   "source": [
    "Let's try naively tokenizing by splitting up the text according to blankspace, using a basic Python string method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99fabb62-ff4e-48f7-b057-f72983dcf87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'little',\n",
       " 'example,',\n",
       " \"we're\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'some',\n",
       " 'of']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use split() to tokenize a string\n",
    "tokens = example2.split()\n",
    "\n",
    "# Print first ten tokens\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa6b33-e927-4adc-8d6b-9a5a1f310e62",
   "metadata": {},
   "source": [
    "We can roughly think of this as \"word tokenization\". However, it's not always clear that simply splitting up by spaces will get what we want. Consider contractions, for example, which really consist of two words connected together. More advanced tokenizations will actually treat these words differently.\n",
    "\n",
    "We will largely be using a package called Natural Language Toolkit, or `nltk`, to perform these operations. `nltk` has a function called `word_tokenize` which can tokenize a string for us in an intelligent fashion. Ultimately, `nltk` basically is a bunch of regular expressions under the hood. \n",
    "\n",
    "There are a host of natural language processing packages one can use. For example, one newer package is `spaCy`, which is extremely powerful. Our goal here is to not make you an expert in a variety of NLP packages, but to expose you to principles that are shared by all of them. In this way, you'll be better prepared to open up any new NLP package you might have to use.\n",
    "\n",
    "Next, we need to install a couple packages within `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e5060d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06c63c1d-e1b3-4f21-aa57-460771e60801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f8769c1-6446-465b-a9b3-06e26086a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'little', 'example', ',', 'we', \"'re\", 'going', 'to', 'see', 'some', 'of', 'the', 'problems', 'that', 'regularly', 'appear', 'in', 'tokenization', '.', 'Tokenization', 'may', 'seem', 'simple', ',', 'but', 'it', \"'s\", 'harder', 'than', 'it', 'first', 'appears', '.', 'Why', 'is', 'it', 'so', 'hard', '?', 'Punctuations', ',', 'contractions', '(', 'like', 'do', \"n't\", ',', 'wo', \"n't\", 'and', 'would', \"'ve\", ')', 'get', 'in', 'the', 'way', '.', 'What', 'do', 'you', 'do', 'when', 'you', 'have', '#', 'hashtags', ',', '@', 'TwitterHandles', ',', 'or', 'https', ':', '//urls.com', '?', 'Different', 'packages', 'will', 'make', 'different', 'decisions', 'on', 'when', 'to', 'split', 'text', 'apart', ',', 'and', 'when', 'not', 'to', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022de3b-ab72-4616-bba2-831f58491757",
   "metadata": {},
   "source": [
    "Looking at this example, you can see how `nltk` has made certain decisions about where and when to tokenize. Tokenization is critical for downstream processing, and there's a variety of methods for performing the tokenizing. Let's take a look at `spaCy`'s tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12cdd905-96ef-42f4-852f-a39ceede2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install spaCy if necessary\n",
    "# %pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "59e4dbd6-46fa-477b-af86-63913670123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "07f98810-7494-48bf-9d01-0c9df9bfc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the dictionary\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Pass the example into the English pipeline\n",
    "doc = nlp(example2)\n",
    "\n",
    "# Store tokens in a variable\n",
    "spacy_tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8acb52a-e49b-4fdd-b09c-2e6c4fd5d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'little', 'example', ',', 'we', \"'re\", 'going', 'to', 'see', 'some', 'of', 'the', 'problems', 'that', 'regularly', 'appear', 'in', 'tokenization', '.', 'Tokenization', 'may', 'seem', 'simple', ',', 'but', 'it', \"'s\", 'harder', 'than', 'it', 'first', 'appears', '.', 'Why', 'is', 'it', 'so', 'hard', '?', 'Punctuations', ',', 'contractions', '(', 'like', 'do', \"n't\", ',', 'wo', \"n't\", 'and', 'would', \"'ve\", ')', 'get', 'in', 'the', 'way', '.', 'What', 'do', 'you', 'do', 'when', 'you', 'have', '#', 'hashtags', ',', '@', 'TwitterHandles', ',', 'or', 'https', ':', '//urls.com', '?', 'Different', 'packages', 'will', 'make', 'different', 'decisions', 'on', 'when', 'to', 'split', 'text', 'apart', ',', 'and', 'when', 'not', 'to', '.']\n",
      "['In', 'this', 'little', 'example', ',', 'we', \"'re\", 'going', 'to', 'see', 'some', 'of', 'the', 'problems', 'that', 'regularly', 'appear', 'in', 'tokenization', '.', 'Tokenization', 'may', 'seem', 'simple', ',', 'but', 'it', \"'s\", 'harder', 'than', 'it', 'first', 'appears', '.', 'Why', 'is', 'it', 'so', 'hard', '?', 'Punctuations', ',', 'contractions', '(', 'like', 'do', \"n't\", ',', 'wo', \"n't\", 'and', 'would', \"'ve\", ')', 'get', 'in', 'the', 'way', '.', 'What', 'do', 'you', 'do', 'when', 'you', 'have', '#', 'hashtags', ',', '@TwitterHandles', ',', 'or', 'https://urls.com', '?', 'Different', 'packages', 'will', 'make', 'different', 'decisions', 'on', 'when', 'to', 'split', 'text', 'apart', ',', 'and', 'when', 'not', 'to', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Compare NLTK to spaCy\n",
    "print(nltk_tokens)\n",
    "\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2147b-841e-4a1f-97f1-fd64f23eb118",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 3: Tokenizing a Large Text\n",
    "\n",
    "Tokenize \"Sowing and Reaping\", which we imported at the beginning of this workshop. Use a method of your choice.\n",
    "\n",
    "Once you've tokenized, find all the unique words types (you might want the `set` function). Then, sort the resulting `set` object to create a vocabulary (you might want to use the `sorted` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b1678051-ecc8-4210-9547-4884598a048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61259cad-3b35-4ce9-89b3-77ba8283bc18",
   "metadata": {},
   "source": [
    "## Removing Stop Words\n",
    "\n",
    "Text often has words that are very common and usually not informative. These words tend to be pronouns or articles, such as \"the\", \"a\", \"it\", \"them\", etc. In many cases, these \"stop words\" are those that we may wish to remove before performing computation since they usually are not very informative. \n",
    "\n",
    "In practice, this is simple to do - we just filter out tokens by words. However, we may want to use different \"stop word lists\", depending on our use case. For example, `nltk` has a stop word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7989a296-4f9c-44eb-b0f1-c554194fc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6bf9ff5f-73c8-446c-be8e-dabe1bd6d8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n"
     ]
    }
   ],
   "source": [
    "# What kinds of words are in the list?\n",
    "print(stop[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c49a372c-ecef-412f-bdf3-34f76f8b3f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'little', 'example,', \"we're\", 'going', 'see', 'problems', 'regularly', 'appear', 'tokenization.', 'Tokenization', 'may', 'seem', 'simple,', 'harder', 'first', 'appears.', 'Why', 'hard?', 'Punctuations,', 'contractions', '(like', \"don't,\", \"would've)\", 'get', 'way.', 'What', '#hashtags,', '@TwitterHandles,', 'https://urls.com?', 'Different', 'packages', 'make', 'different', 'decisions', 'split', 'text', 'apart,', 'to.']\n"
     ]
    }
   ],
   "source": [
    "# Remove tokens that are stop words\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "95d66cdf-89a4-49b6-812d-7215f97272d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this little example, we're going to see some of the problems that regularly appear in tokenization. Tokenization may seem simple, but it's harder than it first appears. Why is it so hard? Punctuations, contractions (like don't, won't and would've) get in the way. What do you do when you have #hashtags, @TwitterHandles, or https://urls.com? Different packages will make different decisions on when to split text apart, and when not to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare to the original text\n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7692ad-8545-435f-b0cd-80f13ffff209",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization both refer to removing morphological affixes on words. Many words consist of a \"core\" word with a modified ending that adjusts the word's meaning in a given context. For example, the word \"grows\" is simply \"grow\" with an \"s\" added to denote a change in verb tense. In many cases, we're interested in the core content of the word. Stemming and lemmatization are the process of getting at the \"core\" of a word. This \"core\" component is often referred to as the *lemma*.\n",
    "\n",
    "Stemming is a rudimentary approach to obtaining the lemma: it simply removes an ending of a word. So, \"grows\" would be stemmed to \"grow\". The word \"running\" would be stemmed to \"run\".\n",
    "\n",
    "Lemmatization is more general: it aims to find the lemma of a word, but can handle cases where stemming may not work. For example, the word \"fairies\" cannot be stemmed to the lemma, \"fairy\". So, we need additional rules - provided by lemmatization - that can appropriately turn \"fairies\" into \"fairy\".\n",
    "\n",
    "`nltk` provides many algorithms for stemming. We'll use the Snowball Stemmer, which we'll import from `nltk`. We'll also look at the Word Net Lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "174b304d-5d5b-4303-9262-c43f6c6948e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e9e5f2e-2384-4631-b260-8fbe1e6970cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the stemmer and lemmatizer\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dac5719f-83d2-49ef-b1a6-3c9cfe0a7ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "run\n",
      "code\n"
     ]
    }
   ],
   "source": [
    "# Stemming examples\n",
    "print(stemmer.stem('grows'))\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('coded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35604e03-b724-4896-85be-7a664d18c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairi\n",
      "wolv\n",
      "abaci\n",
      "leav\n",
      "carri\n"
     ]
    }
   ],
   "source": [
    "# When does stemming not quite work?\n",
    "print(stemmer.stem('fairies'))\n",
    "print(stemmer.stem('wolves'))\n",
    "print(stemmer.stem('abaci'))\n",
    "print(stemmer.stem('leaves'))\n",
    "print(stemmer.stem('carried'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7d46f1f8-e6aa-48db-a954-508fdf4f542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairy\n",
      "wolf\n",
      "abacus\n",
      "leaf\n",
      "carried\n"
     ]
    }
   ],
   "source": [
    "# Let's try lemmatizing these, instead:\n",
    "print(lemmatizer.lemmatize('fairies'))\n",
    "print(lemmatizer.lemmatize('wolves'))\n",
    "print(lemmatizer.lemmatize('abaci'))\n",
    "print(lemmatizer.lemmatize('leaves'))\n",
    "print(lemmatizer.lemmatize('carried'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044da5ed-cf6a-4114-b4b7-102e8504ce42",
   "metadata": {},
   "source": [
    "What happened with that last one? Sometimes we need to provide the lemmatizer a 'part-of-speech' tag to help resolve ambiguous cases. This is another argument in the lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50976d9b-5151-4dc1-862d-6cb2576c8dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carry\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('carried', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6c43f-33e9-4f48-8812-f77e31a85111",
   "metadata": {},
   "source": [
    "Try it with \"leaves\", which has more than one way to lemmatize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "961a0ed8-d653-41c6-932d-90dba75329d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf\n",
      "leave\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('leaves', pos='n'))\n",
    "print(lemmatizer.lemmatize('leaves', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a4cf0-cfd9-4116-94ef-58ef4426b482",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 5: Apply a Lemmatizer to Text\n",
    "\n",
    "Lemmatize the tokenized `example2` text using the `nltk`'s `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc687676-4298-4a88-9655-2e77b72b4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429b117-1983-45bf-bdff-d6f0f83a8c22",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 6: Putting it All Together\n",
    "\n",
    "Write a function called `preprocess()` that accepts a string and performs the following preprocessing steps:\n",
    "\n",
    "* Lowercase text.\n",
    "* Replace all URLs and numbers with their respective tokens.\n",
    "* Strip blankspace.\n",
    "* Tokenize.\n",
    "* Remove punctuation.\n",
    "* Remove stop words.\n",
    "* Lemmatize the tokens.\n",
    "\n",
    "Apply this function to `sowing_and_reaping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f5dbfb0-97b1-4abf-845c-17a0d4942831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8d259147-ad68-417c-99a6-1fecfb9b9a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I hear that John Andrews has given up his saloon; and a foolish thing\\nit was. He was doing a splendid business. What could have induced him?\"\\n\\n\"They say that his wife was bitterly opposed to the business. I don\\'t\\nknow, but I think it quite likely. She has never seemed happy since John\\nhas kept saloon.\"\\n\\n\"Well, I would never let any woman lead me by the nose. I would let her\\nknow that as the living'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sowing_and_reaping[:400]\n",
    "print(\"\\n\")\n",
    "preprocess(sowing_and_reaping[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49653f-8700-4681-a50a-43bbb14c124d",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "# ðŸŽ¬ **Demo**: Powerful Features of `spaCy`\n",
    "\n",
    "We will end this portion of the workshop by examining some of the more powerful features offered by the newer NLP library, `spaCy`. Beside being quite fast, `spaCy` provides very powerful built-in tools in its tokenizer. For example, we automatically get many of the above operations in one fell swoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f741be70-2e03-4b95-b1a0-d5a525da9ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: We; Lemma: we; Part-of-speech: PRON; Token shape: Xx; Alphabetical? True; Stop Word? True\n",
      "Token: 're; Lemma: be; Part-of-speech: AUX; Token shape: 'xx; Alphabetical? False; Stop Word? True\n",
      "Token: learning; Lemma: learn; Part-of-speech: VERB; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: about; Lemma: about; Part-of-speech: ADP; Token shape: xxxx; Alphabetical? True; Stop Word? True\n",
      "Token: natural; Lemma: natural; Part-of-speech: ADJ; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: language; Lemma: language; Part-of-speech: NOUN; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: processing; Lemma: processing; Part-of-speech: NOUN; Token shape: xxxx; Alphabetical? True; Stop Word? False\n",
      "Token: at; Lemma: at; Part-of-speech: ADP; Token shape: xx; Alphabetical? True; Stop Word? True\n",
      "Token: Berkeley; Lemma: Berkeley; Part-of-speech: PROPN; Token shape: Xxxxx; Alphabetical? True; Stop Word? False\n",
      "Token: .; Lemma: .; Part-of-speech: PUNCT; Token shape: .; Alphabetical? False; Stop Word? False\n"
     ]
    }
   ],
   "source": [
    "short_example = \"We're learning about natural language processing at Berkeley.\"\n",
    "doc = nlp(short_example)\n",
    "\n",
    "for token in doc:\n",
    "    print(\n",
    "        f\"Token: {token.text}; Lemma: {token.lemma_}; Part-of-speech: {token.pos_}; \"\n",
    "        f\"Token shape: {token.shape_}; Alphabetical? {token.is_alpha}; Stop Word? {token.is_stop}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec4cf7-eb3c-4b4a-9008-bb2c113634c8",
   "metadata": {},
   "source": [
    "Tokenizing, lemmatization, part of speech tagging, stop word detection, and a couple other things are provided to us up front when we pass a text into the `nlp` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c70884-da79-45e8-a3b1-905f57241817",
   "metadata": {},
   "source": [
    "`spaCy` also comes with some pretty shiny visualization tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ab7351a4-b05f-4938-894f-9a21309b3473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"01b06f35aacd47a8a4fdd4c89a112a32-0\" class=\"displacy\" width=\"1400\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">We</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">'re</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">about</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">processing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">Berkeley.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,102.0 350.0,102.0 350.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-1\" stroke-width=\"2px\" d=\"M212,152.0 212,127.0 347.0,127.0 347.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,154.0 L208,146.0 216,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-2\" stroke-width=\"2px\" d=\"M362,152.0 362,127.0 497.0,127.0 497.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M497.0,154.0 L501.0,146.0 493.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-3\" stroke-width=\"2px\" d=\"M662,152.0 662,127.0 797.0,127.0 797.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,154.0 L658,146.0 666,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-4\" stroke-width=\"2px\" d=\"M812,152.0 812,127.0 947.0,127.0 947.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M812,154.0 L808,146.0 816,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-5\" stroke-width=\"2px\" d=\"M512,152.0 512,102.0 950.0,102.0 950.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M950.0,154.0 L954.0,146.0 946.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-6\" stroke-width=\"2px\" d=\"M962,152.0 962,127.0 1097.0,127.0 1097.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1097.0,154.0 L1101.0,146.0 1093.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-7\" stroke-width=\"2px\" d=\"M1112,152.0 1112,127.0 1247.0,127.0 1247.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01b06f35aacd47a8a4fdd4c89a112a32-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1247.0,154.0 L1251.0,146.0 1243.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", options={'compact': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9e25d-7f90-4656-804e-351af0cf0356",
   "metadata": {},
   "source": [
    "For longer texts, we also get the ability to perform a variety of other operations very easily. Here are some cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "65eee367-1ade-4b95-b5bb-21968a9050b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example3_path = '../data/example3.txt'\n",
    "\n",
    "with open(example3_path, 'r') as file:\n",
    "    example3 = file.read()\n",
    "    \n",
    "doc = nlp(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "64d59d76-d30c-44fb-b862-c208c3212e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-Lab helps Berkeley faculty, staff, and students move forward with world-class research in data intensive social science. We think of data as an expansive category, one that is constantly changing as the research frontier moves. We offer a venue for methodological exchange from all corners of campus and across its bounds. \n",
      "\n",
      "D-Lab provides cross-disciplinary resources for in-depth consulting and advising access to staff support and training and provisioning for software and other infrastructure needs. Networking with other Berkeley centers and facilities and with our departments and schools, we offer our services to researchers across the disciplines and underwrite the breadth of excellence of Berkeleyâ€™s graduate programs and faculty research. D-Lab builds networks that Berkeley researchers can connect with users of social science data in the off-campus world.\n"
     ]
    }
   ],
   "source": [
    "print(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "45ce243b-60b8-4e28-a69b-858e9df3bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Segmentation\n",
      "D-Lab helps Berkeley faculty, staff, and students move forward with world-class research in data intensive social science.\n",
      "We think of data as an expansive category, one that is constantly changing as the research frontier moves.\n",
      "We offer a venue for methodological exchange from all corners of campus and across its bounds. \n",
      "\n",
      "\n",
      "D-Lab provides cross-disciplinary resources for in-depth consulting and advising access to staff support and training and provisioning for software and other infrastructure needs.\n",
      "Networking with other Berkeley centers and facilities and with our departments and schools, we offer our services to researchers across the disciplines and underwrite the breadth of excellence of Berkeleyâ€™s graduate programs and faculty research.\n",
      "D-Lab builds networks that Berkeley researchers can connect with users of social science data in the off-campus world.\n",
      "\n",
      "Entity Detection:\n",
      "Berkeley GPE\n",
      "Berkeley ORG\n",
      "Berkeley ORG\n",
      "Berkeley ORG\n",
      "\n",
      "Noun Chunks:\n",
      "D-Lab\n",
      "Berkeley faculty\n",
      "staff\n",
      "students\n",
      "world-class research\n",
      "data intensive social science\n",
      "We\n",
      "data\n",
      "an expansive category\n",
      "that\n",
      "the research frontier moves\n",
      "We\n",
      "a venue\n",
      "methodological exchange\n",
      "all corners\n",
      "campus\n",
      "its bounds\n",
      "D-Lab\n",
      "cross-disciplinary resources\n",
      "in-depth consulting\n",
      "access\n",
      "staff support\n",
      "training\n",
      "provisioning\n",
      "software\n",
      "other infrastructure needs\n",
      "other Berkeley centers\n",
      "facilities\n",
      "our departments\n",
      "schools\n",
      "we\n",
      "our services\n",
      "researchers\n",
      "the disciplines\n",
      "the breadth\n",
      "excellence\n",
      "Berkeleyâ€™s graduate programs\n",
      "faculty research\n",
      "D-Lab\n",
      "networks\n",
      "that\n",
      "Berkeley researchers\n",
      "users\n",
      "social science data\n",
      "the off-campus world\n"
     ]
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "print('Sentence Segmentation')\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "\n",
    "# Entity detection\n",
    "print('\\nEntity Detection:')\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# Noun chunks\n",
    "print('\\nNoun Chunks:')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f379d0-edf6-4282-9075-cc168b7444a3",
   "metadata": {},
   "source": [
    "There's a whole lot else we can do with it! Check out `spaCy`'s documentation to see more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
