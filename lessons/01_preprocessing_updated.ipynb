{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e7ea21-6437-48e8-a9e4-3bdc05f709c9",
   "metadata": {},
   "source": [
    "# Python Text Analysis Part 1: Preprocessing\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Learn common and task-specific operations of preprocessing.\n",
    "* Know commonly used NLP packages and what they are capable of.\n",
    "* Understand the differences between tokenizers before and after LLMs.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br> \n",
    "\n",
    "### Sections\n",
    "1. [Preprocessing](#section1)\n",
    "2. [Tokenization](#section2)\n",
    "\n",
    "In this three-part workshop series, we will learn the building blocks for performing text analysis in Python. These techniques lie in the domain of Natural Language Processing (NLP). NLP is a field that deals with identifying and extracting patterns of language, primarily in written texts. Throughout the workshop series, we will interact with various packages for performing text analysis: starting from simple string methods to specific NLP packages, including `nltk`, `spaCy`, and more recent ones on Large Language Models (`transformers`).\n",
    "\n",
    "Now, let's have these packages properly installed before diving into the materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d442e4c7-e926-493d-a64e-516616ad915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the following packages\n",
    "# %pip install NLTK\n",
    "# %pip install transformers\n",
    "# %pip install spaCy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b8f8e-4e69-426e-a202-ec48b325e89a",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "In Part 1 of this workshop, we address the first step of text analysis. Our goal is to convert the raw, messy text data into a consistent format. This ensures that our text data preserves the necessary information for subsequent analysis while stripping away less relevant details. This process is often called **preprocessing**, **text cleaning**, or **text normalization**.\n",
    "\n",
    "You will notice that at the end of preprocessing, our data is still in a format that we can read and understand. In Parts 2 and 3, we will begin our foray into converting the text data into a numerical representation---a format that can be handled by computational analysis. \n",
    "\n",
    "üîî **Question**: Let's pause for a minute to reflect on previous experiences working on text data. \n",
    "- What is the format of the text data you have interacted with (CSV, XML, or plain text)?\n",
    "- Where does it come from (structured corpus, scraped from the web, survey data)?\n",
    "- Is it messy (i.e., is the data formatted consistently)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35911a-3b3f-4a48-a7d1-9882aab04851",
   "metadata": {},
   "source": [
    "## Common Processes\n",
    "\n",
    "Preprocessing is not something we can accomplish in one swoop with a single line of code. We often start by familiarizing ourselves with the data, and along the ways we become clearer about the granularity of preprocessing we want to arrive at.\n",
    "\n",
    "Typically, we begin with a set of commonly used processes to clean the data. These operations will not substantially alter the form or meaning of the data; instead, the goal is to convert the text data into a standard format. \n",
    "\n",
    "Oftentimes, these operations can be done with built-in Python functions, such as `string` methods and Regular Expressions. For example, the following processes are commonly applied to Enlish texts. \n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "- Remove stop words\n",
    "\n",
    "Afterwards, we may choose to perform task-specific preprocessing operations, which are dependent on the specific text-analysis task we want to perform later and the source of data where we retrieve our data at the first place. \n",
    "\n",
    "Before we jump into these operations, let's take a look at our data first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d7350-9a1e-4db9-b828-a87fe1676d8d",
   "metadata": {},
   "source": [
    "### Import the Text Data\n",
    "\n",
    "The text data we will be working with is a .csv file. It contains tweets about U.S. airlines, scrapped from Feb 2015. \n",
    "\n",
    "Let's read in the file `airline_tweets.csv` with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1ff64b-53ad-4eca-b846-3fda20085c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# File path to data\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "\n",
    "# Specify the separator to be comma\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e397ac6a-c2ba-4cce-8700-b36b38026c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b339f-45cf-465d-931c-05f9096fd510",
   "metadata": {},
   "source": [
    "The dataframe has one row per tweet. The `text` column contains the tweet, and others metadata of the tweet:\n",
    "\n",
    "- `airline_sentiment` (`str`): sentiment of the tweet, labeled as \"neutral\", \"positive\", or \"negative\". \n",
    "- `airline` (`str`): the airline that is tweeted about.\n",
    "- `retweet count` (`int`): how many times the tweet was retweeted.\n",
    "- `text` (`str`): the text of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c695b-4bd1-4151-9cb9-ef5253eb16df",
   "metadata": {},
   "source": [
    "Let's take a look at some of the Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b690daab-7be5-4b8f-8af0-a91fdec4ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n"
     ]
    }
   ],
   "source": [
    "print(tweets['text'].iloc[0])\n",
    "print(tweets['text'].iloc[1])\n",
    "print(tweets['text'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc05fa-ad30-4402-ab56-086bcb09a166",
   "metadata": {},
   "source": [
    "üîî **Question**: What you have noticed? What are the characteristics of tweet data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3460393-00a6-461c-b02a-9e98f9b5d1af",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "While we need to acknowledge that the **casing** of words is informative, we often don't work in contexts where we can properly utilize this information.\n",
    "\n",
    "More often, the subsequent analysis we perform is **case-insensitive**. For instance, in frequency analysis, we look for word counts want to account for various forms of the same word. Lowercasing the text data aids in this process and simplifies our analysis.\n",
    "\n",
    "We can easily achieve text lowercasing with the built-in string function [`lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower). See [string methods](https://www.w3schools.com/python/python_ref_string.asp) for more useful functions.\n",
    "\n",
    "Let's take a look at the following example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a95d90-3ef1-4bff-9cfe-d447ed99f252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica I was scheduled for SFO 2 DAL flight 714 today. Changed to 24th due weather. Looks like flight still on?\n"
     ]
    }
   ],
   "source": [
    "# Print the first example\n",
    "first_example = tweets['text'][108]\n",
    "print(first_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66d91c0-6eed-4591-95fc-cd2eae2e0d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "==================================================\n",
      "@virginamerica i was scheduled for sfo 2 dal flight 714 today. changed to 24th due weather. looks like flight still on?\n",
      "==================================================\n",
      "@VIRGINAMERICA I WAS SCHEDULED FOR SFO 2 DAL FLIGHT 714 TODAY. CHANGED TO 24TH DUE WEATHER. LOOKS LIKE FLIGHT STILL ON?\n"
     ]
    }
   ],
   "source": [
    "# Check if the example is all lowercased\n",
    "print(first_example.islower())\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Convert it to lowercase\n",
    "print(first_example.lower())\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Convert it to uppercase\n",
    "print(first_example.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0d8c8-bd6c-47ef-b305-09ac61d07d4d",
   "metadata": {},
   "source": [
    "### Remove Extra Whitespace Characters\n",
    "\n",
    "Sometimes we might come across texts with extraneous whitespace. This is particularly common when the text is scrapped from webpages. Before we dive into the details, let's briefly introduce Regular Expressions (regexes) and the `re` package. \n",
    "\n",
    "Regular expressions are a powerful way of searching for specific string patterns in large corpora. They have an infamously steep learning curve, but are very efficient when we get a handle on them. Many NLP packages make heavy use of regexes under the hood. Regex testers are a useful tool in both understanding and creating regex expression. An example is [regex101](https://regex101.com).\n",
    "\n",
    "Our goal in this workshop is not to provide a deep (or even shallow) dive into regexes; instead, we want to expose you to them so that you are better prepared to do deep dives in the future!\n",
    "\n",
    "The following example is a poem by T. S. Eliot. Like many other poems, the text may contain extra line breaks (or newline characters, `\\n`) that we want to remove.\n",
    "\n",
    "Let's read the data in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224b8001-8a47-4af9-aee2-b1bbb415ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the poem\n",
    "text_path = '../data/morning_at_the_window.txt'\n",
    "\n",
    "# Read the poem in\n",
    "with open(text_path, 'r') as file:\n",
    "    text = file.read()\n",
    "    text_excerpt = text[660:]\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a693dd9-9706-40b3-863f-f568020245f7",
   "metadata": {},
   "source": [
    "The output of the text excerpt is the poem, which is in a continuous string of text whith line breaks placed at the end of each line, making it difficult to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e78a75a-8e15-4bcb-a416-783aa7f60ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Morning at the Window\\n\\nThey are rattling breakfast plates in basement kitchens,\\nAnd along the trampled edges of the street\\nI am aware of the damp souls of housemaids\\nSprouting despondently at area gates.\\n\\nThe brown waves of fog toss up to me\\nTwisted faces from the bottom of the street,\\nAnd tear from a passer-by with muddy skirts\\nAn aimless smile that hovers in the air\\nAnd vanishes along the level of the roofs.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_excerpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cce993-c315-4aaa-87fe-149de8607f65",
   "metadata": {},
   "source": [
    "One handy function we can use to display the poem properly is `splitlines()`. As the name suggests, it splits a long text sequence into a list of lines, effectively getting rid of line boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddeade7a-065d-49e6-bdd3-87a8ea8f6e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Morning at the Window',\n",
       " '',\n",
       " 'They are rattling breakfast plates in basement kitchens,',\n",
       " 'And along the trampled edges of the street',\n",
       " 'I am aware of the damp souls of housemaids',\n",
       " 'Sprouting despondently at area gates.',\n",
       " '',\n",
       " 'The brown waves of fog toss up to me',\n",
       " 'Twisted faces from the bottom of the street,',\n",
       " 'And tear from a passer-by with muddy skirts',\n",
       " 'An aimless smile that hovers in the air',\n",
       " 'And vanishes along the level of the roofs.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the single string into a list of lines\n",
    "text_excerpt.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3825b-0857-44e1-bf6a-d8c7a9032704",
   "metadata": {},
   "source": [
    "Let's return to our tweets data for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53a81ea9-65c4-474a-8530-35393555d1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the second example\n",
    "second_example = tweets['text'][5]\n",
    "second_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef55865-36fd-4c06-a765-530cf3b53096",
   "metadata": {},
   "source": [
    "Now, for the tweets data, we do not really want to split it into strings. We'd still expect a single string of text but would like to remove the line breaks completely from the string.\n",
    "\n",
    "The string method `strip()` effectively does the job of stripping away spaces at both ends of the text. However, it won't work in our example as the newline character is within the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b933503b-4370-4dc4-b287-6dc2f9cdb1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strip only removed blankspace at both ends\n",
    "second_example.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b80b4-804f-460f-a2d5-adbd654902b3",
   "metadata": {},
   "source": [
    "This is where regex could be really helpful.\n",
    "\n",
    "Let's load the package in first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceac9714-7053-4b2e-affb-71f8c3d2dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f08d20-ba81-4e48-9e2a-5728148005b3",
   "metadata": {},
   "source": [
    "Now, with regex, we are essentially calling it to match a pattern we identify in the text data, and we want to do some operations on the match---extract it, replace it with something else, or remove it completely. Therefore, how regex works could be unpacked into the following executable steps:\n",
    "\n",
    "- Identify the pattern and write the pattern in regex: `r''`\n",
    "- Write the replacement for the pattern \n",
    "- Call the specific regex function\n",
    "\n",
    "In our example, the pattern we are looking for is `\\s`, which is the regex short name for any whitespace character (`\\n` and `\\t` included). We also add a quantifier in the end `\\s+`, which means the pattern repeats one or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1248d227-1149-4014-94a5-c05592a27a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern in regex\n",
    "blankspace_pattern = r'\\s+'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc075c2e-1a1d-4393-a3ea-8ad7c118364b",
   "metadata": {},
   "source": [
    "The replacement we have for one or more whitespace characters is exactly one single whitespace---which is the canonical word boundary in English. Any more whitespace will be reduced to one single whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c55cb2f1-f4ca-4b79-900c-f65ec303ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a replacement for the pattern identfied\n",
    "blankspace_repl = ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12e3d1-728a-429b-9c83-4dcc88590bc4",
   "metadata": {},
   "source": [
    "Now, in the final step, let's put everything together with `re.sub`: substitute a pattern with a replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5249b24b-7111-4569-be29-c40efa5e148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA\n"
     ]
    }
   ],
   "source": [
    "# Replace whitespace with ' '\n",
    "clean_text = re.sub(blankspace_pattern, blankspace_repl, second_example)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895fbe3-a034-4124-94af-72a528913c51",
   "metadata": {},
   "source": [
    "Ta-da! The newline character is no longer there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087dc0c-5fef-4f1c-8662-7cbc8a978f34",
   "metadata": {},
   "source": [
    "### Remove Punctuation Marks\n",
    "\n",
    "Sometimes we might only be interested in **alphanumeric characters** (i.e., the letters and numbers), in which case we might want to remove punctuation marks. This process becomes less common when we consider more advanced NLP algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70e8502b-b703-45e0-8852-0c3210363440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Load in a predefined list of punctuation marks\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91119c9e-431c-42cb-afea-f7e607698929",
   "metadata": {},
   "source": [
    "In practice, we can iterate over the text and remove characters found in the punctuation list, such as shown below in the `remove_punct` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "237d868d-339d-4bbe-9a3b-20fa5fbdf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc768b-c2dd-4386-8212-483c4485e4be",
   "metadata": {},
   "source": [
    "Let's apply the function to the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7596c465-3d85-4b72-a853-f2151bcd91df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica why are your first fares in May over three times more than other carriers when all seats are available to select???\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VirginAmerica why are your first fares in May over three times more than other carriers when all seats are available to select'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the third example\n",
    "third_example = tweets['text'][20]\n",
    "print(third_example)\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Apply the function \n",
    "remove_punct(third_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a4b83-f503-4405-aedd-66bbc088e3e7",
   "metadata": {},
   "source": [
    "Let's give it a try with another tweet. What have you noticed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b3c2f60-fc92-4326-bad6-5ad04be50476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica trying to add my boy Prince to my ressie. SF this Thursday @VirginAmerica from LAX http://t.co/GsB2J3c4gM\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VirginAmerica trying to add my boy Prince to my ressie SF this Thursday VirginAmerica from LAX httptcoGsB2J3c4gM'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print another tweet\n",
    "print(tweets['text'][100])\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Apply the function\n",
    "remove_punct(tweets['text'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af02ce5-b674-4cb4-8e08-7d7416963f9c",
   "metadata": {},
   "source": [
    "How about the following example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f8c3947-e6b8-42fe-8a58-15e4b6c60005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weve got quite a bit of punctuation here dont we Python DLab'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a text with contraction\n",
    "contraction_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"\n",
    "\n",
    "# Apply the function\n",
    "remove_punct(contraction_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62574c66-db3f-4500-9c3b-cea2f3eb2a30",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** In many cases, we want to remove punctuation **after** tokenization, which we will discuss in the next section. This tells us that the **order** of preprocessing operations is a matter of importance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6b85e-58e7-4f56-9b4a-b60c85b394ba",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Preprocessing with Multiple Steps\n",
    "\n",
    "So far we've learned a few preprocessing operations, let's put them together in a function! \n",
    "\n",
    "The example text data for challenge 1 has been read in. Write a function to:\n",
    "\n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deb10cba-239e-4856-b56d-7d5eb850c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "challenge1_path = '../data/example1.txt'\n",
    "\n",
    "with open(challenge1_path, 'r') as file:\n",
    "    challenge1 = file.read()\n",
    "    \n",
    "print(challenge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2480823-65dd-4f52-a7b3-6d9b10d87912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "    \n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "034a23e3-6e40-44a4-a122-c706dfb44cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a text file that has some extra blankspace at the start and end blankspace is a catchall term for spaces tabs newlines and a bunch of other things that computers distinguish but to us all look like spaces tabs and newlines the python method called strip only catches blankspace at the start and end of a string but it wont catch it in the middle for example in this sentence once again regular expressions will help us with this'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution to challenge 1 \n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = remove_punct(text)\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c159cb-8eaa-4c30-b8ff-38a712d2bb0f",
   "metadata": {},
   "source": [
    "## Task-specific Processes\n",
    "\n",
    "Now that we understand common preprocessing processes, there are still a few additional operations to consider. Our text data might require further normalization depending on the language, source, and content of the data.\n",
    "\n",
    "For example, if we are dealing with financial documents, we might want to standardize monetary symbols by converting them to digits. In the airline tweet data we've been using, there are numerous hashtags and URLs. These can be replaced with placeholders to simplify subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2936cea-74e9-40c2-bfbe-6ba8129330de",
   "metadata": {},
   "source": [
    "### üé¨ **Demo**: Remove Hashtags and URLs \n",
    "\n",
    "Although URLs, hashtags, and numbers are informative in their own right, oftentimes we don't necessarily care about the exact meaning of each of them. \n",
    "\n",
    "While we could remove them completely, it's often informative to know that there **exists** a URL or a hashtag. So, we replace individual URLs and hashtags with a \"symbol\" that preserves the fact these structures exist in the text. It's standard to just use the strings \"URL\" and \"HASHTAG\".\n",
    "\n",
    "Since these types of text often contain precise structure, they're an apt case for using regular expressions. Let's apply these patterns to the Tweets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03c0dc37-a013-4f0a-b72f-a1f64dc6c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel http://t.co/ahlXHhKiyn\n"
     ]
    }
   ],
   "source": [
    "# Print the example tweet \n",
    "url_tweet = tweets['text'][13]\n",
    "print(url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef61bea-ea11-468d-8176-a2f63659d204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel  URL \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL \n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "url_repl = ' URL '\n",
    "re.sub(url_pattern, url_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea8e0f2a-460e-4088-aa89-dc2a8bc6f7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica @virginmedia I'm flying your HASHTAG  HASHTAG  skies again! U take all the HASHTAG  away from travel http://t.co/ahlXHhKiyn\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hashtag\n",
    "hashtag_pattern = r'(?:^|\\s)[ÔºÉ#]{1}(\\w+)'\n",
    "hashtag_repl = ' HASHTAG '\n",
    "re.sub(hashtag_pattern, hashtag_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d68d49-4923-49c0-9113-b844dc7546b9",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "## Tokenizers before LLMs\n",
    "\n",
    "One of the most important steps in text analysis is tokenization. This is the process of breaking down the text into \"tokens,\" which are distinct chunks that we recognize as unique in whatever corpus we're working in.\n",
    "\n",
    "Once we have broken down a sequence of text into individual tokens, we are ready to perform word-level analysis. For instance, we can filter out tokens that do not contribute to the core meaning of the text.\n",
    "\n",
    "In this section, we will introduce how to perform tokenization with `nltk` and `spaCy`, as well as tokenization with a Large Language Model (`bert`). The purpose is to expose you to different NLP packages, understand what functionalities each of them provide, and how to access functions within each.\n",
    "\n",
    "### `nltk`\n",
    "\n",
    "The first package we will be using is called **Natural Language Toolkit**, or `nltk`. \n",
    "\n",
    "Let's install a couple modules within the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "441d81f8-361e-4273-bd36-91a272f4a38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79b699-c3a5-489f-9b3c-95653aba34d6",
   "metadata": {},
   "source": [
    "`nltk` has a function called `word_tokenize`, which tokenizes a string for us in an intelligent fashion. \n",
    "\n",
    "It takes one argument, which is the text to be tokenized, and returns a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b5d6944-c641-4fac-a239-5947a496371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\n"
     ]
    }
   ],
   "source": [
    "# Load word_tokenize in\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Print the example\n",
    "text = tweets['text'][7]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95fde2a3-e4e2-4e61-ad54-e4d5d0a6ba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'a',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " 'there',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the NLTK tokenizer\n",
    "nltk_tokens = word_tokenize(text)\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ead039-7721-4b22-8590-0d7824631675",
   "metadata": {},
   "source": [
    "Here we are, with a list of tokens identified by `nltk`. Let's take a minute to inspect the tokens. Do the word boundaries decided by `nltk` make sense to you? Pay attention to the twitter handle and the URL in the example tweet. \n",
    "\n",
    "You may feel that accessing functions in `nltk` is pretty straightforward. The function we used just now was imported from the `nltk.tokenize` module, which as the name suggests, primarily does the job of tokenization!\n",
    "\n",
    "Underlyingly, `nltk` has [a collection of modules](https://www.nltk.org/api/nltk.html) that fulfill different purposes, to name a few:\n",
    "\n",
    "| NLTK module   | Fucntion                  | Link                                                         |\n",
    "|---------------|---------------------------|--------------------------------------------------------------|\n",
    "| nltk.tokenize | Tokenization              | [Documentation](https://www.nltk.org/api/nltk.tokenize.html) |\n",
    "| nltk.corpus   | Retrieve built-in corpora | [Documentation](https://www.nltk.org/nltk_data/)             |\n",
    "| nltk.tag      | Part-of-speech tagging    | [Documentation](https://www.nltk.org/api/nltk.tag.html)      |\n",
    "| nltk.stem     | Stemming                  | [Documentation](https://www.nltk.org/api/nltk.stem.html)     |\n",
    "| ...           | ...                       | ...                                                          |\n",
    "\n",
    "Let's import `stopwords` from the `nltk.corpus` module, which hosts various built-in corpora available in `nltk`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84bbfced-8803-41ca-9cae-49bdadf8c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a list of predefined stop words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee971a1-1189-4cb6-8317-4836f54c3ae2",
   "metadata": {},
   "source": [
    "Let's specificy that we want to retrieve English stop words. The function simply returns a list of stop words, mostly function words, that `nltk` identifies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6009e1df-b720-4d22-a162-7fd250a58672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 10 stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec908-de6c-42c5-a370-f1b1df0032b3",
   "metadata": {},
   "source": [
    "### `spaCy`\n",
    "Other than `nltk`, we have another widely-used package called `spaCy`. Functions in `spaCy` are organized differently.\n",
    "\n",
    "`spaCy` has its own processing pipeline. It takes in a string of text, runs the `nlp` pipeline on it, and stores the processed text and its annotations in an object called `doc`. The `nlp` pipeline always performs tokenization, followed by [a number of components](https://spacy.io/usage/processing-pipelines#custom-components) as specified by the user. These components are, in fact, pretty similar to the modules in the `nltk` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0facd-4b75-41ac-920c-5ea044f7ae2e",
   "metadata": {},
   "source": [
    "<img src='../images/spacy.png' alt=\"spacy pipeline\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef1eaf-2790-4928-b094-943f2803c6a0",
   "metadata": {},
   "source": [
    "Note that we always start by initializing the `nlp` pipeline, depending on the language of the text. Here, we are loading a pretrained language model for English: `en_core_web_sm`. It means the model is trained on a small set of web text data.\n",
    "\n",
    "This is the first time we introduce the concept of **pretraining**, though you may have heard it elsewhere. In the context of NLP, pretraining means that the tools we are using are trained, or in other words, optimized on tons of English text. Therefore, when we apply them to our own data, we can expect them to be somewhat efficient and accurate. Much of the functionality offered in `spaCy`, as outlined below, relies on pretraining.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "524dfc02-aa8f-4888-9f81-74a570db72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d669c3-2f5a-41b6-893b-ea1d438b3a48",
   "metadata": {},
   "source": [
    "The `nlp` pipeline by default includes a number of components, which we can access via the `pipe_names` attribute. \n",
    "\n",
    "You may notice that it dosen't include the tokenizer. Don't worry! Tokenizer is a specicial component that the pipeline always includes, thus it is not counted towards added components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d581ca5-43f8-4ef9-b099-2fc92c324581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve components included in NLP pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e37f91-d174-4101-bfc6-2859cb0fe5cc",
   "metadata": {},
   "source": [
    "Let's run the `nlp` pipeline on our example tweet data, and assign it to a variable `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "618e8558-625d-4546-8109-63f9bae9790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to example tweet\n",
    "doc = nlp(tweets['text'][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54325d60-5c5c-488d-baf2-7eed4de2c031",
   "metadata": {},
   "source": [
    "Under the hood, the `doc` object contains the tokens (done by the tokenizer) and their annotations (done by other components), which are [linguistic features](\n",
    "https://spacy.io/usage/linguistic-features) useful for text analysis. We retrieve the token and its annotations by accessing its attributes. \n",
    "\n",
    "| Attribute      | Annotation                              | Link                                                                      |\n",
    "|----------------|-----------------------------------------|---------------------------------------------------------------------------|\n",
    "| token.text     | the token in text                       | [Documentation](https://spacy.io/api/token#attributes)                    |\n",
    "| token.is_stop  | whether the token is a stop word        | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.is_punct | whether the token is a punctuation mark | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.lemma_   | the base form of the token              | [Documentation](https://spacy.io/usage/linguistic-features#lemmatization) |\n",
    "| token.pos_     | the simple POS-tag of the token         | [Documentation](https://spacy.io/usage/linguistic-features#pos-tagging)   |\n",
    "| ...            | ...                                     | ...                                                                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c71efee-e6cf-46c4-9198-593304f6560d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'a',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " 'there',\n",
       " '.',\n",
       " 'https://t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the verbatim texts of tokens\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "\n",
    "# The tokens are in a list\n",
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4fc23f0-c699-45e6-ad62-e131036d601f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'a',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " 'there',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the NLTK tokens\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ace59e-40e0-42b3-9f2b-d30ac94dccab",
   "metadata": {},
   "source": [
    "üîî **Question**: Let's pause for a minute to compare the tokens from `nltk` and `spaCy`. What have you noticed?\n",
    "\n",
    "`spaCy` conveniently encodes whether a token is part of the stop word list as an annotation, making it more straightforward for us to check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "626af687-e986-4c97-af86-edf7dbd22c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the is_stop annotation\n",
    "spacy_stops = [token.is_stop for token in doc]\n",
    "\n",
    "# The results are boolean values\n",
    "spacy_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6548b6-7e89-4f42-b8cb-bf7c93b34eb4",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Remove Stop Words\n",
    "\n",
    "We have known how `nltk` and `spaCy` work as NLP packages, now let's write two functions to remove stop words from our text data. \n",
    "- Complete the function for stop words removal using `nltk`\n",
    "    - Requires two arguments: the tokenized text and a list of stop words\n",
    "- Complete the function for stop words removal using `spaCy`\n",
    "    - Requires one argument: the doc object which contains the processed text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "350875b5-e7e4-48d3-a03e-9b5ddb7ca8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_nltk(text, stopword):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    text = [token for token in text if token not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a72b7efd-c88f-4632-a66b-a57428e156b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_spacy(doc):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    doc = [token.text for token in doc if token.is_stop is False]\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4c3b0da-2223-4d7f-9014-696498e804e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopword_nltk(word_tokenize(text), stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f83538ba-6bf1-49ca-90ec-b6532b1ffcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica',\n",
       " 'missed',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'Men',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " '.',\n",
       " 'https://t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopword_spacy(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6b1ec-87cc-4a08-a5dd-0210a9c56f0b",
   "metadata": {},
   "source": [
    "## üé¨ **Demo**: Powerful Features from `spaCy`\n",
    "\n",
    "As mentioned above, `spaCy`'s nlp pipeline includes a number of linguistic annotations, which could be very useful for text analysis. \n",
    "\n",
    "For instance, we can access more annotations such as the lemma, the part-of-speech tag and its meaning, and whether the token looks like URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb6c7d93-51a3-4fb8-8321-cb672f4f1b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica           | @VirginAmerica           | PROPN        | proper noun  | 0            |\n",
      "Really                   | really                   | ADV          | adverb       | 0            |\n",
      "missed                   | miss                     | VERB         | verb         | 0            |\n",
      "a                        | a                        | DET          | determiner   | 0            |\n",
      "prime                    | prime                    | ADJ          | adjective    | 0            |\n",
      "opportunity              | opportunity              | NOUN         | noun         | 0            |\n",
      "for                      | for                      | ADP          | adposition   | 0            |\n",
      "Men                      | Men                      | PROPN        | proper noun  | 0            |\n",
      "Without                  | without                  | ADP          | adposition   | 0            |\n",
      "Hats                     | Hats                     | PROPN        | proper noun  | 0            |\n",
      "parody                   | parody                   | NOUN         | noun         | 0            |\n",
      ",                        | ,                        | PUNCT        | punctuation  | 0            |\n",
      "there                    | there                    | ADV          | adverb       | 0            |\n",
      ".                        | .                        | PUNCT        | punctuation  | 0            |\n",
      "https://t.co/mWpG7grEZP  | https://t.co/mWpG7grEZP  | PROPN        | proper noun  | 1            |\n"
     ]
    }
   ],
   "source": [
    "# Print tokens and their annotations\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<24} | {token.lemma_:<24} | {token.pos_:<12} | {spacy.explain(token.pos_):<12} | {token.like_url:<12} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17388e0c-88b6-4cd9-8d2b-adb7f10b5330",
   "metadata": {},
   "source": [
    "As you can imagine, it is typical for this dataset to contain place names and airport codes. It would be cool if we are able to identify them and extract them from tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb489f78-fbb2-497b-a36d-3400c00c9b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@JetBlue Vegas, San Francisco, Baltimore, San Diego and Philadelphia so far! I'm a very frequent business traveler.\n",
      "==================================================\n",
      "@VirginAmerica Flying LAX to SFO and after looking at the awesome movie lineup I actually wish I was on a long haul.\n"
     ]
    }
   ],
   "source": [
    "# Print example tweets with place names and airport codes\n",
    "tweet_city = tweets['text'][8273]\n",
    "tweet_airport = tweets['text'][502]\n",
    "print(tweet_city)\n",
    "print(f\"{'=' * 50}\")\n",
    "print(tweet_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013990a5-5e07-4a45-9427-fcd33840d3b8",
   "metadata": {},
   "source": [
    "We can use the \"ner\" (Named Entity Recognition) component to identify entities and their categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9e63519-5991-49fa-9a5d-be9f9b408ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vegas           | 9          | 14         | GPE       \n",
      "San Francisco   | 16         | 29         | GPE       \n",
      "Baltimore       | 31         | 40         | GPE       \n",
      "San Diego       | 42         | 51         | GPE       \n",
      "Philadelphia    | 56         | 68         | GPE       \n"
     ]
    }
   ],
   "source": [
    "# Print entities identified from the text\n",
    "doc_city = nlp(tweet_city)\n",
    "for ent in doc_city.ents:\n",
    "    print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b933ed0-7018-450c-b0a6-fb76cb6d5be9",
   "metadata": {},
   "source": [
    "We can also use `displacy` to highlight entities identified in the text, and at the same time, annotate the entity category. \n",
    "\n",
    "In the following example, we have four `GPE` (i.e., geopolitical entities, usually countries and cities) identified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a5a5219-af1f-445c-a35b-7c49d739a91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">@JetBlue \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vegas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Baltimore\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Diego\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Philadelphia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " so far! I'm a very frequent business traveler.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the identified entities\n",
    "from spacy import displacy\n",
    "displacy.render(doc_city, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7953b-04a1-46d0-817a-db29edd8c83b",
   "metadata": {},
   "source": [
    "Let's give it a try with another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9246c7e3-8990-4d47-a355-deb63dbd1cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica  | 0          | 14         | CARDINAL  \n",
      "Flying LAX      | 15         | 25         | ORG       \n",
      "SFO             | 29         | 32         | ORG       \n"
     ]
    }
   ],
   "source": [
    "# Print entities identified from the text\n",
    "doc_airport = nlp(tweet_airport)\n",
    "for ent in doc_airport.ents:\n",
    "     print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df38472-3193-44c5-9b2f-e311ce9d42e0",
   "metadata": {},
   "source": [
    "Interesting that airport codes are identified as `ORG`--- orgnizations, and the tweet handle as `CARDINAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e4bf382-4c57-4b78-a37f-fc1f6cb1d565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    @VirginAmerica\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Flying LAX\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SFO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and after looking at the awesome movie lineup I actually wish I was on a long haul.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the identified entities\n",
    "displacy.render(doc_airport, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d6f28-effc-4fe1-90e9-47c89dc5492d",
   "metadata": {},
   "source": [
    "## Tokenizers since LLMs\n",
    "\n",
    "So far, we've seen what tokenization looks like with two widely-used NLP packages. They work quite well in some settings, but not others. Recall that NLTK struggles with URLs. Now, imagine the data we have is even messier, containing misspellings, recently coined words, foreign names, and etc (collectively called \"out of vocabulary\" or OOV words). In such circumstances, we might need a more powerful model to handle these complexities.\n",
    "\n",
    "In fact, tokenization schemes change substantially with **Large Language Models** (LLMs), which are models trained on vast amounts of data from mixed sources. With that massive amount of data, LLMs are better at chunking a longer sequence into tokens and tokens into **subtokens**. These subtokens could be morphological units of a word, such as a prefix, but they could also be parts of a word where the model sets a \"meaningful\" boundary. \n",
    "\n",
    "In this section, we will demonstrate tokenization in **BERT** (Bidirectional Encoder Representations from Transformers), which utilizes a tokenization algorithm called [**WordPiece**](https://huggingface.co/learn/nlp-course/en/chapter6/6). \n",
    "\n",
    "We will load the tokenizer of BERT from the package `transformers`, which hosts a number of transformer-based LLMs (e.g., GPT-2). We will not go depth into Transformer in this workshop, but feel free to check out the D-lab workshop on [GPT Fundamentals](https://github.com/dlab-berkeley/GPT-Fundamentals)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1509b-5b9e-456d-909f-a6b5099c48c8",
   "metadata": {},
   "source": [
    "### WordPiece tokenization\n",
    "\n",
    "Note that BERT comes in a variety of versions. The one we will explore today is `bert-base-uncased`. This model has moderate size (referred to as `base`) and is case-insensitive, meaning the input text will be converted to lowercase by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7927226c-d04e-4117-9c49-3d355611b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingyu/anaconda3/envs/dlab-text-analysis/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cfb38-274e-4d75-9d14-8744020fe49c",
   "metadata": {},
   "source": [
    "The tokenizer has multiple functions, as we will see in a minute. Now we want to access the `.tokenize()` function from the tokenizer. \n",
    "\n",
    "Let's tokenize an example tweet below, what have you noticed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62649193-bb00-4ae8-9102-bce3d1dfb6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: @VirginAmerica Just DM'd. Same issue persisting.\n",
      "==================================================\n",
      "Tokens: ['@', 'virgin', '##ame', '##rica', 'just', 'd', '##m', \"'\", 'd', '.', 'same', 'issue', 'persist', '##ing', '.']\n",
      "Number of tokens: 15\n"
     ]
    }
   ],
   "source": [
    "# Select an example tweet from dataframe\n",
    "text = tweets['text'][194]\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Apply tokenizer\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f780e-207c-4d3d-b1b6-063c6d118945",
   "metadata": {},
   "source": [
    "The double \"hashtag\" symbols (`##`) mean that the token is a subword, which is a segment of a longer token.\n",
    "\n",
    "üîî **Question**: Do these subwords make sense to you? \n",
    "\n",
    "One significant development with LLMs is that each token is assigned an ID in its vocabulary. This is important because computational analysis does not operate directly on strings of text. Our computer does not understand text in its raw form, so each token is translated to an ID. These IDs are the inputs that the model can access and operate.\n",
    "\n",
    "Tokens and IDs can be converted bidirectionally, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3112a63d-82b6-4fc0-a904-ab86f8740653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of just is: 2074\n",
      "Token 2074 is: just\n"
     ]
    }
   ],
   "source": [
    "# Get the input ID of the word \n",
    "print(f\"ID of just is: {tokenizer.vocab['just']}\")\n",
    "\n",
    "# Get the text of the input ID\n",
    "print(f\"Token 2074 is: {tokenizer.decode([2074])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fd13d-f26e-480c-b43e-2b5fbc4898cd",
   "metadata": {},
   "source": [
    "Let's convert tokens to input IDs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d125e1d-2560-4136-829c-b1c11e34636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs of text: [1030, 6261, 14074, 14735, 2074, 1040, 2213, 1005, 1040, 1012, 2168, 3277, 29486, 2075, 1012]\n",
      "The number of input IDs: 15\n"
     ]
    }
   ],
   "source": [
    "# Convert a list of tokens to a list of input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Input IDs of text: {input_ids}\")\n",
    "print(f\"The number of input IDs: {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a1a14-a6db-414b-a1a8-c6be73c81a15",
   "metadata": {},
   "source": [
    "### Special tokens\n",
    "\n",
    "In addition to the tokens and subtokens discussed above, BERT also makes use of three special tokens: `SEP`, `CLS`, and `UNK`. The `SEP` token acts as a sentence terminator, commonly known as an `EOS` (End of Sentence) token. The `UNK` token represents any token that is not found in the vocabulary, hence \"unknown\" tokens. The `CLS` token is automatically added to the beginning of the sentence. It originates from classification tasks, where people found it useful to have a token that aggregates the information of the entire sentence for classification purposes.\n",
    "\n",
    "When we apply `tokenizer()` directly to our text data, we are asking BERT to **encode** the text for us. This involves multiple steps: \n",
    "- Tokenize the text\n",
    "- Add special tokens\n",
    "- Convert tokens to input IDs\n",
    "- Other model-specific processes\n",
    "\n",
    "The results are kept in a dictionary. \n",
    "\n",
    "Let's print them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57161cc5-92e6-4e7c-abbd-2171132c4b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [101, 1030, 6261, 14074, 14735, 2074, 1040, 2213, 1005, 1040, 1012, 2168, 3277, 29486, 2075, 1012, 102]\n",
      "\n",
      "\n",
      "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The results of encoding are formatted as a dictionary\n",
    "for item, values in tokenizer(text).items():\n",
    "    print(item+':', values)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eeb66-aadb-4b29-b4db-9e7b1d3029bc",
   "metadata": {},
   "source": [
    "You may have noticed that we have two special input IDs added: 101 and 102. \n",
    "\n",
    "Let's convert them to texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21479c25-7a9a-4fac-ba09-1812575b8170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of input IDs: 17\n",
      "IDs from tokenizer: [101, 1030, 6261, 14074, 14735, 2074, 1040, 2213, 1005, 1040, 1012, 2168, 3277, 29486, 2075, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# We can also get the input IDs by providing the key \n",
    "input_ids_from_tokenizer = tokenizer(text)['input_ids']\n",
    "print(f\"The number of input IDs: {len(input_ids_from_tokenizer)}\")\n",
    "print(f\"IDs from tokenizer: {input_ids_from_tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82a18dc1-ca0d-4d5b-8ac6-f56cb44bf8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 101st token: [CLS]\n",
      "The 102nd token: [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Convert input IDs to texts\n",
    "print(f\"The 101st token: {tokenizer.convert_ids_to_tokens(101)}\")\n",
    "print(f\"The 102nd token: {tokenizer.convert_ids_to_tokens(102)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56be32-2a5f-441e-8283-de3e60705c0b",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Find the Word Boundary\n",
    "\n",
    "Now that we know BERT tokenization would often return subwords. Let's try a few more examples! \n",
    "\n",
    "Does the result make sense to you? What do you think is the correct word boundary to split the following words into subwords? \n",
    "\n",
    "Also feel free to read more about limitations of the WordPiece algorithm. For instance, [this blog post](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99) dives into why would it fail, and [this one](https://tinkerd.net/blog/machine-learning/bert-tokenization/#demo-bert-tokenizer) introduces the mechanism underlying the algoritm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1140b19-398c-44dd-829c-c922b0e6f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c07c71e-5be2-4d91-9c6f-26de4145307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dl', '##ab']\n",
      "['co', '##vid']\n",
      "['hug', '##ga', '##ble']\n",
      "['37', '##8']\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations\n",
    "get_tokens('dlab')\n",
    "\n",
    "# OOV\n",
    "get_tokens('covid')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# YOUR EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0812a7-f033-46ed-bc7b-67109c369e6c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Preprocessing includes multiple steps, some of them are more common to text data regardlessly, and some are task-specific. \n",
    "* Both `nltk` and `spaCy` could be used for tokenization and stop word removal. The latter is more powerful in providing more useful linguistic features. \n",
    "* Tokenization works differently in BERT, which often involves breaking down a whole word into subwords. \n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
