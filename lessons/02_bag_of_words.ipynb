{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Bag of Words\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Apply string manipulations to text data. \n",
    "* Use both default python packages and NLP-specific packages to do text cleaning. \n",
    "* Learn to build a pipeline of preprocessing and tokenization. \n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "ðŸ”” **Question**: A quick question to help you understand what's going on.<br>\n",
    "ðŸ¥Š **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "ðŸ’¡ **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "ðŸŽ¬ **Demo**: Showing off something more advanced â€“ so you know what Python can be used for!<br> \n",
    "\n",
    "### Sections\n",
    "1. [Importing Text Data](#section1)\n",
    "2. [Text Cleaning](#section2)\n",
    "3. [Tokenization](#section3)\n",
    "4. [Demo: Powerful Features of `spaCy`](#demo)\n",
    "\n",
    "In the previous lesson, we learned how to use several packages to preprocess text. However, we never moved beyond the *text representation* - we only manipulated the representation itself. If we want to perform computational analysis on the text, we will need to devise approaches to convert the text into a *numeric representation*.\n",
    "\n",
    "In this lesson, we'll explore one of the simplest ways to generate a numeric representation from text: the **bag-of-words**. With this numeric representation, we'll build a simple classifier and explore what the classifier tells us. At the heart of the bag-of-words approach lies the hypothesis that the presence and frequency of specific tokens is informative about the semantics and sentiment behind the text.\n",
    "\n",
    "We'll make heavy use of the `scikit-learn` package to do so, as it provides a nice framework for constructing the numeric representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4dbc3-52dd-4dc9-8d0d-21ad31808d09",
   "metadata": {},
   "source": [
    "# The Airline Tweets Dataset\n",
    "\n",
    "We'll work with a dataset consisting of tweets about US airlines. Each sample is a different tweet, which was posted at a specific airline. These tweets may express positive, neutral, or negative \"sentiment\". Our eventual goal will be to predict the sentiment of the tweet given its text.\n",
    "\n",
    "The dataset was collected by Crowdflower, which they then made public through Kaggle. We've already downloaded it and placed it in the `data` directory. Note that this dataset, on the whole, is structured nicely and has already undergone some cleaning. However, this is not the norm in real-life data science! We've chosen this dataset so that we can concentrate on learning and understanding the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd13fe4-37b0-4a66-8cee-d45f61a8f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/mingyu/anaconda3/envs/dlab-text-analysis/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/mingyu/anaconda3/envs/dlab-text-analysis/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mingyu/anaconda3/envs/dlab-text-analysis/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mingyu/anaconda3/envs/dlab-text-analysis/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/mingyu/anaconda3/envs/dlab-text-analysis/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install nltk if you haven't already \n",
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4a3a0d-66f4-44e5-8dd6-5f441146014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mingyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f537df-169a-4a08-bced-f9687238654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4190e351-97b7-4c5b-866e-07aa6cbd42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "# Use pandas to import tweets\n",
    "tweets_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e8a36-bd58-4c24-8593-03a0ea70deed",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Before we ever do any preprocessing or modeling, we always should do some exploratory data analysis to get a feel for the dataset.\n",
    "\n",
    "First, let's take a look at the first few rows and all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "We have a `tweet_id`, which uniquely identifies each tweet. We also have an `airline_sentiment`, which takes on values of `\"positive\"`, `\"negative\"`, or `\"neutral\"`. There are other columns indicating the author of the tweet, when it was created, the timezone of the user, and others. The main column of interest is the `text` column: these are the tweets. Let's take a look at a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n",
      "@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "@VirginAmerica and it's a really big bad thing about it\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    print(tweets['text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "We can already see that some of these tweets very obviously negative sentiment - how can you tell this is the case? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400a77f-7e79-44bf-81dc-40bf8953a6d1",
   "metadata": {},
   "source": [
    "Let's take a look at which airlines are tweeted about and how many of each in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e98f355-830d-4d48-a1ad-16b1d8b29d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airlines = tweets['airline']\n",
    "sns.countplot(x=airlines, order=airlines.value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c497c-4138-47eb-b959-235b552e7a47",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 1: Getting to Know the Data\n",
    "\n",
    "Use `pandas` to find out the following about the airline tweets:\n",
    "\n",
    "* How many tweets are in the dataset?\n",
    "* How many tweets are positive, neutral, and negative?\n",
    "* What *proportion* of tweets are positive, neutral, and negative?\n",
    "* Make a bar plot showing the proportion of tweet sentiments.\n",
    "\n",
    "If you have time, try the following:\n",
    "\n",
    "* How much time separates the earliest and latest tweets?\n",
    "* What gets more retweets: positive, negative, or neutral tweets?\n",
    "* Identify the airline whose tweets have the highest proportion of negative sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d80b3-4842-4ab4-871d-212ac7e87f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We spent much of the last workshop learning how to preprocess the data. Let's apply what we learned to this dataset. Looking at some of the tweets above, we can see that while they are in pretty good shape, we can do some additional processing on them.\n",
    "\n",
    "In our pipeline, we'll omit the tokenization process, since we will perform it in a later step. Instead, we'll add a couple new preprocessing steps now that we are working with social media data. Specifically, we'll replace all hashtags with a \"HASHTAG\" token, and we'll replace all users (denoted by the \"@\" symbol) with a \"USER\" token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 2: Creating a Preprocessing Pipeline for Social Media Data\n",
    "\n",
    "Write a function called `preprocess()` that performs the following on a text input:\n",
    "\n",
    "* Lowercase text.\n",
    "* Replace all URLs with the token \"URL\".\n",
    "* Replace all numbers with the token \"DIGIT\".\n",
    "* Replace hashtags with the token \"HASHTAG\".\n",
    "* Replace all users with the token \"USER\".\n",
    "* Remove blankspaces.\n",
    "\n",
    "We have provided regex patterns for each of the replacement steps in the following cells.\n",
    "\n",
    "Run your `preprocess()` function on `example_tweet` (two cells below), and when you think you have it working, apply it to the entire `text` column in the tweets DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb499df1-b1ce-41f8-a41b-3a3fbf51bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful regex patterns\n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "digit_pattern = '\\d+'\n",
    "hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "user_pattern = r'@(\\w+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75bb1fab-3ad6-486d-81f1-b294cf5e653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your function to the following example\n",
    "example_tweet = \"lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33052a00-679c-4f78-b934-1634c144d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(text):\n",
    "#     # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    \n",
    "#     return text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728c43fb-13aa-4b1b-a99b-4a92f9404520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lol USER and USER are like soo DIGIT HASHTAG HASHTAG saw it on URL HASHTAG'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on example tweet\n",
    "preprocess(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f7bb6a-f064-48cc-b650-12c4ef2fbb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 USER what USER said.\n",
       "1    USER plus you've added commercials to the expe...\n",
       "2    USER i didn't today... must mean i need to tak...\n",
       "3    USER it's really aggressive to blast obnoxious...\n",
       "4        USER and it's a really big bad thing about it\n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply to text column to create a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acc6-b305-492a-8fde-65b343cb779c",
   "metadata": {},
   "source": [
    "Preprocessing is complete - time for the bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "# The Bag-of-Words Representation\n",
    "\n",
    "The fundamental principle behind bag-of-words is to encode the corpus in terms of word frequencies. Consider the case of sentiment: we know sentiment is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy\", it likely conveys positive sentiment (but not always - someone might say they're \"not happy\" - the opposite sentiment!). Furthermore, when those words come up more often, they'll probably more strongly convey the sentiment.\n",
    "\n",
    "In a bag-of-words, we taken some text, tokenize it, and then tabulate the frequencies of each token. The numerical representation of the text, then, is a vector indicating the frequencies of each token for that text.\n",
    "\n",
    "For example, if we're considering a movie review as follows: \n",
    "\n",
    "![bow](../images/bow.png)\n",
    "\n",
    "We take each token from the review, \"toss it in a bag\", and count up the frequencies of each word. The numerical representation, then, is the vector on the right: the number of appearances of each token. The \"bag\" here denotes that we are not modeling structure within the text - only the frequencies of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "In most text corpora, we will have many samples or *documents*. For example, in the airline tweets dataset, we have many tweets. Each tweet stands on its own as a unique sample: they can each be thought of as a unique document in the entire *corpus*. Since they are all related to each other, many tokens might be shared across tweets. So, when creating the bag-of-words model, we can tokenize across all documents, forming a *vocabulary*. Then, we can represent a single document by which of the tokens in the vocabulary are represented, and their frequency within the document.\n",
    "\n",
    "If the vocabulary has $V$ tokens, then each document will be encoded in a $V$-dimensional vector. If there are $D$ documents, the entire dataset can be represented in a $D \\times V$ matrix, where each row corresponds to the document, and each column corresponds to the token (or \"term\"). This $D \\times V$ matrix is a **document term matrix** (DTM).\n",
    "\n",
    "Let's consider a simple example. Suppose we have the \"documents\":\n",
    "\n",
    "```\n",
    "[\"You are at a workshop. Are you ready?\",\n",
    " \"Welcome to Berkeley!\",\n",
    " \"I am teaching a workshop.\"]\n",
    "```\n",
    "\n",
    "The unique (word) tokens in this \"corpus\", in alphabetical order, are:\n",
    "\n",
    "```\n",
    "[a, am, are, at, berkeley, i, ready, teaching, to, welcome, workshop, you]\n",
    "```\n",
    "\n",
    "The DTM can be formed by going through each document, ticking off the frequency of each token in each document, and plugging this number into the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{a} & \\text{am} & \\text{are} & \\text{at} & \\text{berkeley} & \\text{i} & \\text{ready} & \\text{teaching} & \\text{to} & \\text{welcome} & \\text{workshop} & \\text{you} \\\\\\hline\n",
    "\\text{Document 1} & 1 & 0 & 2 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\\\\n",
    "\\text{Document 2} & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "\\text{Document 3} & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The numerical representation for each document is a row in the matrix. For example, Document 1 has numerical representation $[1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 1]$.\n",
    "\n",
    "To create a DTM, we will use the `CountVectorizer` from the package `sklearn`, a heavily used machine learning package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f41838-a387-43d0-b4f7-c2e0dba4d050",
   "metadata": {},
   "source": [
    "If you're not familiar with `sklearn`, here is the general workflow:\n",
    "\n",
    "1. We first create a `CountVectorizer` object, and choose specific settings for how we'll go about creating the DTM. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see what options are available.\n",
    "2. Then, we \"fit\" this `CountVectorizer` object to the data. In this context, \"fitting\" consists of establishing a vocabulary of tokens from the documents in your dataset.\n",
    "3. Finally, we \"transform\" the data according to the \"fitted\" `CountVectorizer` object. This means taking our text data and transforming it into a DTM according to the vocabulary established by the \"fitting\" step.\n",
    "4. You can do steps 2 and 3 in one fell swoop using a `fit_transform` function.\n",
    "\n",
    "Let's start by creating a `CountVectorizer` with the default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f8d2f-c21e-40a8-9ad0-4a8f1a8fe005",
   "metadata": {},
   "source": [
    "Next, we'll fit *and* transform the airline tweets data. What does the documentation say about this function? \n",
    "\n",
    "We need to pass in all the tweets. What is returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f85e76ea-bc54-4775-bcda-432a03d2c96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14640x9913 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 230849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036ca73-2d0b-4781-9f2c-a215c838f4b2",
   "metadata": {},
   "source": [
    "It's not *quite* a `numpy` array. Instead, it's a `numpy` array stored in \"Compressed Sparse Format\". This format saves a lot of memory, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first uncompress this matrix. For larger datasets, however, you'll want to avoid this, as there are performance benefits to using CSF.\n",
    "\n",
    "Converting to a normal `numpy` array is easy: we use a built-in `todense()` function, and pass this into the `np.array()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run if you have limited memory - this includes DataHub and Binder\n",
    "# np.array(counts.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b33def-90fe-4130-9a26-dbee653b9e1c",
   "metadata": {},
   "source": [
    "There are a lot of zeros here! This makes sense: there are probably a lot of tokens in the vocabulary, and each tweet likely only has a few of the tokens.\n",
    "\n",
    "It would be good to know which column refers to which tokens. Let's create a `pandas` DataFrame which has this information. First, we'll need to get the names of each token - how can we do this? Hint: always read the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 9913)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_exact_</th>\n",
       "      <th>_wtvd</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aadavantage</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zukes</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _exact_  _wtvd  aa  aaaand  aadavantage  aadv  aadvantage  aal  aaron  ab  \\\n",
       "0        0      0   0       0            0     0           0    0      0   0   \n",
       "1        0      0   0       0            0     0           0    0      0   0   \n",
       "2        0      0   0       0            0     0           0    0      0   0   \n",
       "3        0      0   0       0            0     0           0    0      0   0   \n",
       "4        0      0   0       0            0     0           0    0      0   0   \n",
       "\n",
       "   ...  zero  zig  zip  zippers  zone  zones  zoom  zukes  zurich  zz  \n",
       "0  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "1  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "2  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "3  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "4  ...     0    0    0        0     0      0     0      0       0   0  \n",
       "\n",
       "[5 rows x 9913 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract tokens\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Run this instead if you scikit-learn version is a bit older\n",
    "# tokens = vectorizer.get_feature_names()\n",
    "# Create DTM\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "# Look at DTM\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb9e39-93c0-4301-bf9d-c4243a12d96b",
   "metadata": {},
   "source": [
    "What can we do with the DTM? For one, we can see the most frequent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user      16500\n",
       "to         8653\n",
       "digit      8427\n",
       "the        6063\n",
       "you        4401\n",
       "for        4001\n",
       "flight     3935\n",
       "on         3815\n",
       "and        3733\n",
       "my         3288\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2e239-4bb8-4452-a358-cc10c46b8786",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 3: DTM Data Analysis\n",
    "\n",
    "* Print out the most infrequent words rather than the most frequent words. If you're not sure how, check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)!\n",
    "* Print the average number of times each word is used in a tweet.\n",
    "* Which non-hashtag, non-digit token appears the most in any given tweet? How many times does it appear? What is the original tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcc05f-22b2-4821-8c6e-ae3d674e948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8e37-4880-4565-b6fc-7e7c96958f0f",
   "metadata": {},
   "source": [
    "## Customizing the `CountVectorizer`\n",
    "\n",
    "Recall the documentation on the `CountVectorizer` object: there are many options here on how you can customize the procedure of generating the vocabulary. Let's highlight some of these values:\n",
    "\n",
    "* `lowercase`: If `True`, it will lowercase all text. This was one of our preprocessing steps from before!\n",
    "* `stop_words`: You can choose or provide a list of stop-words. The `\"english\"` option is a built-in stop-word list, but you could provide another (e.g., the stop-word list provided by `nltk`).\n",
    "* `min_df` or `max_df`: The **document frequency** is the fraction of documents that a token appears in. If this value is high (close to 1), a token appears in most documents. If it's low (close to 0), it appears in very few documents. You might want to remove very rare tokens (they could be typos or gibberish) and you might want to remove very common tokens (these could be uninformative words that aren't stop words). These arguments allow you to specify a range of desired document frequencies. They can either be counts or fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37a0a93e-9dd8-43dc-a82c-06a24bf02bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 5152)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abi</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aadv  aadvantage  aal  abandoned  abc  abi  abilities  ability  able  \\\n",
       "0   0     0           0    0          0    0    0          0        0     0   \n",
       "1   0     0           0    0          0    0    0          0        0     0   \n",
       "2   0     0           0    0          0    0    0          0        0     0   \n",
       "3   0     0           0    0          0    0    0          0        0     0   \n",
       "4   0     0           0    0          0    0    0          0        0     0   \n",
       "\n",
       "   ...  yup  yvonne  yvr  yyj  yyz  zero  zone  zoom  zurich  zz  \n",
       "0  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "1  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "2  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "3  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "4  ...    0       0    0    0    0     0     0     0       0   0  \n",
       "\n",
       "[5 rows x 5152 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95)\n",
    "\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Run this instead if you scikit-learn version is a bit older\n",
    "# tokens = vectorizer.get_feature_names()\n",
    "# Create dataframe\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1efefa-058b-4016-ba88-7e4e3aa65538",
   "metadata": {},
   "source": [
    "How did the number of tokens change with our new parameter settings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5145-d505-4e36-9a39-a40d25d8ec6f",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 4: Customizing the Vectorizer with `nltk` inputs\n",
    "\n",
    "If you look at the `CountVectorizer` documentation, you'll see that it can actually accept a custom `tokenizer` and `stop_words` list. \n",
    "\n",
    "Using what you learned in the previous workshop, create a `CountVectorizer` that utilizes the `nltk` word tokenizer and stop word list. How does the resulting DTM look different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da610560-62c3-48ab-a1b2-25e0b589bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ff597-caa9-4d0b-92b7-57b59156b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363398-fdf5-456b-ae3d-cae9d5294140",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency Scores (TF-IDF)\n",
    "\n",
    "So far, we're relying on word frequencies to give us information about a document. This assumes if a word appears more often in a document, it's more informative. However, this may not always be the case. For example, we already remove stop-words because they are not informative, despite the fact that they appear many times in a document. In the airline tweets, the word `flight` appears many times across the corpus, but is also not that informative, because it appears in many documents. Since we're looking at airline tweets, we shouldn't be surprised to see the word `flight`!\n",
    "\n",
    "To remedy this, we use a weighting scheme called **tf-idf (term frequency-inverse document frequency)**. The big idea behind tf-idf is to weight words not just by their frequency *within* a document, but by their frequency in one document *relative* to the remaining documents. Words that are frequent, but also used in every single document, will not be that informative. We want to identify words that are unevenly distributed across the corpus: these are the words most likely to be informative in a particular document.\n",
    "\n",
    "So, when we construct the DTM, we will be assigning each value a **tf-idf score**. Specifically, term $t$ in document $d$ is assigned tf-idf score as follows:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(d, t) = \\text{tf}(d, t) \\times \\text{idf}(t)\n",
    "$$\n",
    "\n",
    "where $\\text{tf}(d, t)$ is the term-frequency of term $t$ in document $d$ and $\\text{idf}(t)$ is the inverse-document frequency of term $t$. \n",
    "\n",
    "When we used the `CountVectorizer` above, we only considered **term frequency**: if a term appeared more times in a document, it was given a higher value in the DTM. Now, we are scaling the term frequency by the **inverse document frequency**: if a term appears in many documents, we give it a lower weight, since it is less \"surprising\". \n",
    "\n",
    "We're not done yet: we still need to define the actual functions. In the `CountVectorizer`, the `tf(d, t)` function was simply $n_{d,t}$, or the number of times term $t$ appeared in document $d$. However, we can use other functions. In this case, we'll use:\n",
    "\n",
    "$$\n",
    "\\text{tf}(d, t) = \\frac{n_{d,t}}{N_d}\n",
    "$$\n",
    "\n",
    "where $N_d$ is the number of terms in document $d$. All we're doing here is normalizing the term frequency used in the `CountVectorizer`. Next, the inverse document frequency can be calculated as\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = 1 + \\log\\left(\\frac{D}{D_t}\\right)\n",
    "$$\n",
    "\n",
    "where $D$ is the total number of documents, and $D_t$ is the number of documents containing term $t$. If every document contains term $t$, then $\\text{idf}(t) = 1$, and no scaling will happen. If very few documents contain term $t$, then $\\text{idf}(t)$ will be very large, and the term frequency will be scaled up. In practice, the inverse document frequency is computed as \n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = 1 + \\log\\left(\\frac{1 + D}{1 + D_t}\\right)\n",
    "$$\n",
    "\n",
    "to prevent any issues with zero occurrences.\n",
    "\n",
    "We can also create a tf-idf DTM using `sklearn`. We'll use a `TfidfVectorizer` this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5e32d8a-c42d-475f-aab4-21eca8b1aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d23916c1-5693-456c-b71d-6d9d78d1e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14640x9913 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 230849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dtm = vectorizer.fit_transform(tweets['text_processed'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55e509c8-5402-4be0-9143-0e448fff7066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_exact_</th>\n",
       "      <th>_wtvd</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aadavantage</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zukes</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _exact_  _wtvd   aa  aaaand  aadavantage  aadv  aadvantage  aal  aaron  \\\n",
       "0      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "1      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "2      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "3      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "4      0.0    0.0  0.0     0.0          0.0   0.0         0.0  0.0    0.0   \n",
       "\n",
       "    ab  ...  zero  zig  zip  zippers  zone  zones  zoom  zukes  zurich   zz  \n",
       "0  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "1  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "2  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "3  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "4  0.0  ...   0.0  0.0  0.0      0.0   0.0    0.0   0.0    0.0     0.0  0.0  \n",
       "\n",
       "[5 rows x 9913 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = pd.DataFrame(dtm.todense(),\n",
    "                     columns=vectorizer.get_feature_names_out(),\n",
    "                     index=tweets.index)\n",
    "# Run this if your scikit-learn version is older\n",
    "#tfidf = pd.DataFrame(dtm.todense(),\n",
    "#                     columns=vectorizer.get_feature_names(),\n",
    "#                     index=tweets.index)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eb33e-f7dd-4e49-8d2f-eb5d14a36297",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with the highest tf-idf weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b0ced2b-22b2-4ee5-89b2-2fbba93e10e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "whatsoever    0.245656\n",
       "rly           0.246627\n",
       "aftr          0.248886\n",
       "fnd           0.248886\n",
       "hstg          0.248886\n",
       "med           0.248886\n",
       "bng           0.248886\n",
       "cnceld        0.248886\n",
       "dread         0.250462\n",
       "fantasy       0.255245\n",
       "rapidly       0.255245\n",
       "dpted         0.257093\n",
       "connex        0.261642\n",
       "dkyde         0.263007\n",
       "lick          0.267418\n",
       "shoulda       0.268080\n",
       "miscnx        0.269171\n",
       "runners       0.269171\n",
       "awrd          0.269371\n",
       "savr          0.269371\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.max().sort_values(ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da410cb3-a452-441b-a94d-8f751d59d7a6",
   "metadata": {},
   "source": [
    "# Supervised Learning: Sentiment Classification\n",
    "\n",
    "Now that we have a numerical representation of the text, we'd like to *do* something with it. A common task is supervised learning: using the numerical representation to predict some kind of label about the text. Text classification can consist of many types of analysis, such as:\n",
    "\n",
    "* Sentiment analysis\n",
    "* Genre classification\n",
    "* Language identification\n",
    "* Authorship attribution\n",
    "* Spam detection\n",
    "* Document relevancy\n",
    "\n",
    "and many others. How exactly do we go about doing this?\n",
    "\n",
    "Let's consider a toy example:\n",
    "\n",
    "```\"This was the best service! Would love to come again!\"```\n",
    "\n",
    "This is very clearly expressing positive sentiment. But how did we make that judgement?\n",
    "\n",
    "* The review claims the service is the \"best\". This is pretty positive.\n",
    "* The reviewer says they would \"love\" to come again. This is also positive.\n",
    "\n",
    "Specific key words (tokens) were predictive of the sentiment. If someone says something was the \"best\", it's a good sign there's positive sentiment in the text. The semantic meaning of these words helps convey sentiment. \n",
    "\n",
    "This is how we can proceed with classification. We'll construct a DTM from the text data, and use that to predict the labels using a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1121954-e13c-4bdc-9323-6834060d7197",
   "metadata": {},
   "source": [
    "## Predicting Sentiment with Logistic Regression\n",
    "\n",
    "We're going to use a logistic regression model to predict the labels. If you're not familiar with this model, or the basics of machine learning, we recommend taking the Python Machine Learning workshop offered at the D-Lab. For now, we'll review some basics.\n",
    "\n",
    "The DTM is a $D\\times T$ matrix, where $D$ is the number of documents, and $T$ is the number of terms. We can think of the $T$ terms as the \"features\": these values are what we'll use to predict the sentiment. The $D$ documents represent samples: we'll use the patterns across these samples to learn a relationship between the feature and the outcome.\n",
    "\n",
    "In logistic regression, we are learning a **linear model** represented by specific parameters, which we'll call $\\beta_i$. For features $x_i$ (this is a row in the DTM), we construct a **logit**:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "and obtain a probability $p$ by passing this logit through the **sigmoid function**, which maps any value onto the range $[0, 1]$:\n",
    "\n",
    "$$\n",
    "p = \\text{sigmoid}(L) = \\frac{1}{1 + \\exp(-L)}\n",
    "$$\n",
    "\n",
    "We can quite literally think of $p$ as a probability that the sample falls in one of the two classes. If, say, $p>0.5$, we call the sample positive sentiment; otherwise, it's negative sentiment.\n",
    "\n",
    "So, to summarize: we take each sample, which consists of features $x = (x_1, x_2, \\ldots, x_T)$, multiply them by $\\beta_i$ values, and add them up. Pass them through a sigmoid, and get a probability. The key question is: how do we know what $\\beta_i$ values to use? We won't discuss thus details here, but an optimization algorithm will learn the best values given the data. \n",
    "\n",
    "Let's train a model! We're going to use `sklearn` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33413d63-87eb-489f-b374-3cfeaa51cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2af16205-d64f-4eed-a882-c914627844e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did you kernel die from memory issues? \n",
    "# Here's a handy cell to get everything set up from here.\n",
    "# You can restart your kernel and just start from this point. \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load tweets\n",
    "tweets_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text\n",
    "\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "example_tweet = \"lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d33c6-2640-44cd-8a3a-3dd62b98f3af",
   "metadata": {},
   "source": [
    "To understand the theoretical gist of our classification task, let's first focus on a binary 'positive vs negative' classifier. We are going to do so by restricting the analysis to the non-neutral tweets. So, we'll first partition the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "508769ec-dd07-4492-8308-17e031a522da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11541, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "tweets_binary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca64c2-0cfe-4e12-ae44-74c75fae456d",
   "metadata": {},
   "source": [
    "Next, we're going to apply a tf-idf vectorizer to the data. We're going to make use of the `max_features` argument to restrict the number of tokens our model has to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9358ed4d-8108-4fef-9a97-0271544bd30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "dtm = vectorizer.fit_transform(tweets_binary['text_processed'])\n",
    "X = np.asarray(dtm.todense())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6a8e5-da30-4363-8f17-112a0fe94083",
   "metadata": {},
   "source": [
    "Now, we need to get the labels for each of the 11,541 tweets. We can do this by simply extracting the `airline_sentiment` column - `sklearn` is smart enough to handle the details later. Let's also check out the distribution of the labels, to get a sense for a good baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e7df4f3-0fb0-406f-9240-c9f9761eabea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline_sentiment\n",
      "negative    9178\n",
      "positive    2363\n",
      "Name: count, dtype: int64\n",
      "airline_sentiment\n",
      "negative    0.795252\n",
      "positive    0.204748\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y = tweets_binary['airline_sentiment']\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ff74-3fbb-472a-b795-6f4d18fab215",
   "metadata": {},
   "source": [
    "Machine learning fundamentally requires that we learn the $\\beta_i$ values from a **training set**, and then evaluate the performance on a separately held **test set**. This ensures that we actually develop an algorithm that can **generalize**. We'll use the `train_test_split` function from `sklearn` to separate our data into two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64cec8b9-14d9-4897-9c02-cc89fcf7b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066771d8-2f31-4646-9a1b-6d2b1b9b208c",
   "metadata": {},
   "source": [
    "In order to streamline the training process, we've written a `fit_logistic_regression` function you can use to easily train a model given the data inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d46de0b2-af00-4a1d-b4cd-31b96ce545d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        cv=3,\n",
    "        solver='liblinear',\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aa7ea-1bc1-43e2-beeb-0ba2da9b2df9",
   "metadata": {},
   "source": [
    "We'll fit the model, and see how it performs on both the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "773963bd-6603-4fad-884b-09ce60afab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "model = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3abaa8c-cbb9-46cf-925a-0248a638f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9539644714038128\n",
      "Test accuracy: 0.9172802078822001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e186c5-1719-4deb-bdb4-614a9980f058",
   "metadata": {},
   "source": [
    "The model got ~95% accuracy on the training set, and ~92% on the test set - that's pretty good! The similarity between the two performances is also a good sign - it means we were able to generalize pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b5232-b9f5-471b-8269-ee529a6b1072",
   "metadata": {},
   "source": [
    "## Validating Model Performance on New Tweets\n",
    "\n",
    "Now that we have a trained model, there's nothing stopping us from using it on new data! The `model` object comes equipped with a `predict` function that we can use to evaluate on new text samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21c7ea47-4d82-4918-9875-d0520358de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some new tweets\n",
    "new_tweets = [example_tweet,\n",
    "              'omg I am never flying on United again',\n",
    "              'I love @VirginAmerica so much #friendlystaff',\n",
    "              'food on Air France is great!']\n",
    "# First, we need to preprocess them\n",
    "new_tweets_processed = [preprocess(tweet) for tweet in new_tweets]\n",
    "# Next, we need to vectorize them\n",
    "X_new = np.asarray(vectorizer.transform(new_tweets_processed).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf110a09-e99a-40fa-adef-51b1482da413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'positive', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run predictions\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7f31f1f-1eeb-4811-819a-a6ccba08fe59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.95889248e-01, 5.04110752e-01],\n",
       "       [9.38917952e-01, 6.10820479e-02],\n",
       "       [6.71716552e-04, 9.99328283e-01],\n",
       "       [2.49677324e-01, 7.50322676e-01]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get probabilities\n",
    "model.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec2ccf-e339-4f08-bc86-9d2aa42864d6",
   "metadata": {},
   "source": [
    "Those predictions look pretty good! Feel free to give the model a try on other tweets you write."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dac39-4753-4ae8-8dfa-e65e5824cccb",
   "metadata": {},
   "source": [
    "## Interpreting the Model Coefficients\n",
    "\n",
    "The nice thing about logistic regression is that it is **interpretable**. Take a look at the logit again:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "If $L$ is very positive, the sigmoid will make sure that the probability $p$ is close to 1 (positive sentiment). If $L$ is very negative, the sigmoid will make sure that the probability $p$ is close to 0 (negative sentiment). So, by looking at the coefficients $\\beta_i$, we can see how each feature (token) impacts the eventual prediction. If $\\beta_i >0$, then the presence of that feature implies positive sentiment, according to the model. If  $\\beta_i < 0$, then the presence of that feature implies negative sentiment, according to the model.\n",
    "\n",
    "So, let's take a look at the fitted coefficients to see if what we see makes sense! We can access them using the `coef_` member, and we can match each coefficient to the tokens from the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dcb6ef1-13b3-437e-813c-7118911847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model.coef_.ravel()\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "importance = pd.DataFrame()\n",
    "importance['token'] = tokens\n",
    "importance['coefs'] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5598e78-924d-41a5-a442-dc9c32ce2df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>-1.152643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aadv</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aadvantage</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token     coefs\n",
       "0          aa -1.152643\n",
       "1        aadv  0.000000\n",
       "2  aadvantage  0.000000\n",
       "3         aal  0.000000\n",
       "4   abandoned  0.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e63814e-9c0d-4f7a-a5e0-72cca2758d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>worst</td>\n",
       "      <td>-12.531917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>not</td>\n",
       "      <td>-9.801564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>rude</td>\n",
       "      <td>-9.257152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>cancelled</td>\n",
       "      <td>-8.822005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>no</td>\n",
       "      <td>-8.691956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>amazing</td>\n",
       "      <td>10.835885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>awesome</td>\n",
       "      <td>10.885405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>worries</td>\n",
       "      <td>11.673543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>thanks</td>\n",
       "      <td>13.814429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>thank</td>\n",
       "      <td>15.688906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token      coefs\n",
       "4888      worst -12.531917\n",
       "2797        not  -9.801564\n",
       "3771       rude  -9.257152\n",
       "601   cancelled  -8.822005\n",
       "2777         no  -8.691956\n",
       "...         ...        ...\n",
       "144     amazing  10.835885\n",
       "327     awesome  10.885405\n",
       "4884    worries  11.673543\n",
       "4378     thanks  13.814429\n",
       "4375      thank  15.688906\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance.sort_values('coefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed48ea-fd35-4585-9b98-90456aaee447",
   "metadata": {},
   "source": [
    "This makes sense! Tokens like \"worst\", \"not\", and \"rude\" imply negative sentiment, while tokens like \"thanks\", \"kudos\", \"awesome\" imply positive sentiment. Interestingly, the token \"worries\" implies positive sentiment - what does this tell you about how people typically use this token in their tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803473d-9544-4e6d-a840-241cd9f810f6",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 5: Multinomial Logistic Regression\n",
    "\n",
    "Try developing a **multinomial logistic regression** model, to predict positive, negative, and neutral labels. We've provided you a fitter function below, but it's up to you to create new labels, train-test splits, and perform the fitting and evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17933ad2-ddd2-42ab-bebb-d5a344848211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multinomial_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        multi_class='multinomial',\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        cv=3,\n",
    "        solver='saga',\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c9e29-0fbd-4bb2-982c-9b195c30f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efe867-1e98-4e82-8b55-6251061321e5",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 6: Try a Different Classifier\n",
    "\n",
    "Create a new fitter function that uses a `RandomForestClassifier`. How is the performance? Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf07d41-bcf1-4ed2-a699-a53f26a5ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(X, y):\n",
    "    \"\"\"Fits a random forest model to provided data.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
